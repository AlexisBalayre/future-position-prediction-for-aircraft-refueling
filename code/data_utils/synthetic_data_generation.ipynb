{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthetic Data Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook explores the generation of synthetic data to train the future position prediction model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to generate a Background Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def generate_background_image(width, height, num_shapes, shape_size):\n",
    "    \"\"\"\n",
    "    Generate a simple background image with random shapes.\n",
    "\n",
    "    Parameters:\n",
    "    width (int): Width of the background image\n",
    "    height (int): Height of the background image\n",
    "    num_shapes (int): Number of shapes to draw\n",
    "    shape_size (int): Size of each shape\n",
    "\n",
    "    Returns:\n",
    "    img (numpy array): Background image\n",
    "    \"\"\"\n",
    "    img = np.zeros((height, width, 3), dtype=np.uint8)  # Initialize black background\n",
    "\n",
    "    for _ in range(num_shapes):\n",
    "        x = np.random.randint(0, width - shape_size)\n",
    "        y = np.random.randint(0, height - shape_size)\n",
    "        cv2.rectangle(\n",
    "            img,\n",
    "            (x, y),\n",
    "            (x + shape_size, y + shape_size),\n",
    "            (\n",
    "                np.random.randint(0, 255),\n",
    "                np.random.randint(0, 255),\n",
    "                np.random.randint(0, 255),\n",
    "            ),\n",
    "            -1,\n",
    "        )\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "# width, height = 640, 480  # Dimensions of the background image\n",
    "# num_shapes = 10  # Number of shapes to draw\n",
    "# shape_size = 20  # Size of each shape\n",
    "# background_image = generate_background_image(\n",
    "#     width, height, num_shapes, shape_size\n",
    "# )  # Generate background image\n",
    "# cv2.imwrite(\n",
    "#     \"/Users/alexis/Library/CloudStorage/OneDrive-Balayre&Co/Cranfield/Thesis/thesis-github-repository/data/synthetic/background/background1.png\",\n",
    "#     background_image,\n",
    "# )  # Save background image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Camera intrinsic matrix\n",
    "\n",
    "1. Setting Up the Camera Projection\n",
    "   First, define the parameters for the camera:\n",
    "\n",
    "**Intrinsic matrix (K)**: Describes the camera's internal parameters (focal length, optical center).  \n",
    "**Extrinsic matrix (R, T)**: Describes the camera's position and orientation in the world (rotation and translation).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = np.array([[1000, 0, 128], [0, 1000, 128], [0, 0, 1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to project 3D points to 2D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to project 3D points to 2D\n",
    "def project_points(points, K, R, T):\n",
    "    \"\"\"\n",
    "    Project 3D points to 2D using the pinhole camera model.\n",
    "\n",
    "    Parameters:\n",
    "        points: Nx3 array of 3D points\n",
    "        K: Intrinsic matrix\n",
    "        R: Rotation matrix\n",
    "        T: Translation vector\n",
    "    \"\"\"\n",
    "    points_homogeneous = np.hstack((points, np.ones((points.shape[0], 1))))\n",
    "    projection_matrix = K @ np.hstack((R, T.reshape(-1, 1)))\n",
    "    points_2d_homogeneous = points_homogeneous @ projection_matrix.T\n",
    "    points_2d = points_2d_homogeneous[:, :2] / points_2d_homogeneous[:, 2, np.newaxis]\n",
    "    return points_2d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to Generate 3D Linear Motion Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "\n",
    "\n",
    "# Function to generate 3D linear motion dataset\n",
    "def generate_3d_linear_motion_dataset(num_frames, bg_img, output_dir):\n",
    "    \"\"\"\n",
    "    Generate a synthetic dataset with 3D linear motion.\n",
    "\n",
    "    Parameters:\n",
    "        num_frames (int): Number of frames to generate\n",
    "        bg_img (str): Path to the background image\n",
    "        output_dir (str): Output directory to save the frames and annotations\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Rotation (identity matrix, no rotation for simplicity)\n",
    "    R = np.eye(3)\n",
    "    T_initial = np.array(\n",
    "        [0, 0, 1000]\n",
    "    )  # Initial translation (starting far from the camera)\n",
    "\n",
    "    # Initialise the list to store the annotations\n",
    "    annotations = []\n",
    "\n",
    "    for frame_num in range(num_frames):\n",
    "        T = T_initial + np.array([frame_num * 5, 0, -frame_num * 5])\n",
    "\n",
    "        # Define the 3D position of the object (can be a simple representation)\n",
    "        obj_3d_position = np.array(\n",
    "            [[0, 0, 0]]\n",
    "        )  # Object centered at origin in its local 3D coordinate space\n",
    "\n",
    "        # Project the 3D points to 2D\n",
    "        obj_2d_position = project_points(obj_3d_position, K, R, T).astype(int)[0]\n",
    "\n",
    "        # Calculate 2D bounding box\n",
    "        w, h = 50, 50\n",
    "        bbox = (obj_2d_position[0] - w // 2, obj_2d_position[1] - h // 2, w, h)\n",
    "\n",
    "        # Draw the bounding box on the background\n",
    "        img = cv2.imread(bg_img)\n",
    "        cv2.rectangle(\n",
    "            img,\n",
    "            (bbox[0], bbox[1]),\n",
    "            (bbox[0] + bbox[2], bbox[1] + bbox[3]),\n",
    "            (255, 0, 0),\n",
    "            2,\n",
    "        )\n",
    "\n",
    "        # Save the frame\n",
    "        frame_filename = f\"frame_{frame_num:04d}.png\"\n",
    "        frame_filepath = os.path.join(output_dir, frame_filename)\n",
    "        cv2.imwrite(frame_filepath, img)\n",
    "\n",
    "        # Append annotation\n",
    "        annotations.append(\n",
    "            {\"frame\": frame_filename, \"bbox\": [int(coord) for coord in bbox]}\n",
    "        )\n",
    "\n",
    "    # Save the annotations to a JSON file\n",
    "    annotations_filepath = os.path.join(output_dir, \"annotations.json\")\n",
    "    with open(annotations_filepath, \"w\") as f:\n",
    "        json.dump(annotations, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "# bg_img_path = \"/Users/alexis/Library/CloudStorage/OneDrive-Balayre&Co/Cranfield/Thesis/thesis-github-repository/data/synthetic/background/background1.png\"\n",
    "# generate_3d_linear_motion_dataset(\n",
    "#     num_frames=100,\n",
    "#     bg_img=bg_img_path,\n",
    "#     output_dir=\"/Users/alexis/Library/CloudStorage/OneDrive-Balayre&Co/Cranfield/Thesis/thesis-github-repository/data/synthetic/3d_linear_motion_dataset_test1\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to Generate 3D Non-linear Motion Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_frame(\n",
    "    frame_num,\n",
    "    T_initial,\n",
    "    base_size,\n",
    "    bg_img,\n",
    "    output_dir,\n",
    "    z_min,\n",
    "    z_max,\n",
    "    frame_size,\n",
    "    scale_factor,\n",
    "):\n",
    "    \"\"\"\n",
    "    Process a single frame by projecting 3D points into 2D and saving the image with a bounding box.\n",
    "\n",
    "    Paramètres :\n",
    "        frame_num: Frame number\n",
    "        T_initial: Initial translation vector\n",
    "        base_size: Base size of the bounding box\n",
    "        bg_img: Path to background image\n",
    "        output_dir: Directory for saving frames\n",
    "        z_min: Minimum value authorised for the camera's z coordinate\n",
    "        z_max: Maximum authorised value for the camera's z coordinate\n",
    "        frame_size : Size of the video frame\n",
    "        scale_factor: Scaling factor to slow down movement\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Rotation (identity matrix, no rotation for simplicity)\n",
    "        R = np.eye(3)\n",
    "\n",
    "        # Non-linear translation \n",
    "        t = frame_num * scale_factor\n",
    "        \"\"\" T = T_initial + np.array(\n",
    "            [100 * np.cos(t), 100 * np.sin(t), -frame_num * 3 * scale_factor]\n",
    "        ) \"\"\"\n",
    "        T = T_initial + np.array(\n",
    "            [\n",
    "                100 * np.cos(t) + 50 * np.sin(2 * t),\n",
    "                100 * np.sin(t) + 50 * np.cos(2 * t),\n",
    "                -frame_num * 3 * scale_factor,\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Ensures that the Z coordinate is within a valid range\n",
    "        T[2] = np.clip(T[2], z_min, z_max)\n",
    "\n",
    "        # Defines the object's 3D position\n",
    "        obj_3d_position = np.array([[0, 0, 0]])\n",
    "\n",
    "        # Projecting 3D points in 2D\n",
    "        obj_2d_position = project_points(obj_3d_position, K, R, T).astype(int)[0]\n",
    "\n",
    "        # Keeps the object in the frame\n",
    "        obj_2d_position[0] = np.clip(\n",
    "            obj_2d_position[0], base_size // 2, frame_size[0] - base_size // 2\n",
    "        )\n",
    "        obj_2d_position[1] = np.clip(\n",
    "            obj_2d_position[1], base_size // 2, frame_size[1] - base_size // 2\n",
    "        )\n",
    "\n",
    "        # Calculates the size of the 2D bounding box as a function of camera distance\n",
    "        z = T[2]  # Z coordinate of the camera translation vector\n",
    "        size_scaling_factor = base_size / (z / 1000)\n",
    "        w = h = max(\n",
    "            1, int(size_scaling_factor)\n",
    "        )  # Ensures that the width and height are at least 1\n",
    "\n",
    "        # Calculates the 2D bounding box\n",
    "        bbox = (obj_2d_position[0] - w // 2, obj_2d_position[1] - h // 2, w, h)\n",
    "\n",
    "        # Draw the bounding box on the background\n",
    "        img = cv2.imread(bg_img)\n",
    "        if img is None:\n",
    "            raise FileNotFoundError(f\"Image de fond {bg_img} non trouvée\")\n",
    "        cv2.rectangle(\n",
    "            img,\n",
    "            (bbox[0], bbox[1]),\n",
    "            (bbox[0] + bbox[2], bbox[1] + bbox[3]),\n",
    "            (255, 0, 0),\n",
    "            2,\n",
    "        )\n",
    "\n",
    "        # Saves the frame\n",
    "        frame_filename = f\"frame_{frame_num:04d}.png\"\n",
    "        frame_filepath = os.path.join(output_dir, frame_filename)\n",
    "        cv2.imwrite(frame_filepath, img)\n",
    "\n",
    "        return {\"frame\": frame_filename, \"bbox\": [int(coord) for coord in bbox]}\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors du traitement de la frame {frame_num} : {e}\")\n",
    "        return {\"frame\": None, \"bbox\": []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_3d_nonlinear_motion_dataset(\n",
    "    num_frames,\n",
    "    bg_img,\n",
    "    output_dir,\n",
    "    z_min=100,\n",
    "    z_max=1000,\n",
    "    frame_size=(640, 480),\n",
    "    scale_factor=0.01,\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate a synthetic dataset with 3D non-linear motion.\n",
    "\n",
    "    Paramètres:\n",
    "        num_frames (int): Frames number\n",
    "        bg_img (str): Path to background image\n",
    "        output_dir (str): Path to output folder\n",
    "        z_min (int, optional): Minimum value authorised for the camera's z coordinate. Defaults to 100.\n",
    "        z_max (int, optional): Maximum authorised value for the camera's z coordinate. Defaults to 1000.\n",
    "        frame_size (tuple, optional): Size of the video frame. Defaults to (640, 480).\n",
    "        scale_factor (float, optional): Scaling factor to slow down movement. Defaults to 0.01.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    #  Initial translation (initial distance from the camera)\n",
    "    T_initial = np.array([0, 0, z_max])\n",
    "    base_size = 50  # Basic size of the bounding box when z = z_max\n",
    "\n",
    "    # Processing each frame\n",
    "    annotations = []\n",
    "    for frame_num in range(num_frames):\n",
    "        annotation = process_frame(\n",
    "            frame_num,\n",
    "            T_initial,\n",
    "            base_size,\n",
    "            bg_img,\n",
    "            output_dir,\n",
    "            z_min,\n",
    "            z_max,\n",
    "            frame_size,\n",
    "            scale_factor,\n",
    "        )\n",
    "        if annotation[\"frame\"]:\n",
    "            annotations.append(annotation)\n",
    "\n",
    "    # Save the annotations to a JSON file\n",
    "    annotations_filepath = os.path.join(output_dir, \"annotations.json\")\n",
    "    with open(annotations_filepath, \"w\") as f:\n",
    "        json.dump(annotations, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "#     bg_img_path = \"/Users/alexis/Library/CloudStorage/OneDrive-Balayre&Co/Cranfield/Thesis/thesis-github-repository/data/synthetic/background/background1.png\"\n",
    "#     generate_3d_nonlinear_motion_dataset(\n",
    "#         num_frames=10000,\n",
    "#         bg_img=bg_img_path,\n",
    "#         output_dir=\"/Users/alexis/Library/CloudStorage/OneDrive-Balayre&Co/Cranfield/Thesis/thesis-github-repository/data/synthetic/3d_nonlinear_motion_dataset_test6\",\n",
    "#     )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
