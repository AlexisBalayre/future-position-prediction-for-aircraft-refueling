{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Preparation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frames Extraction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "\n",
    "# Function to extract frames from video\n",
    "def extract_frames(video_path, output_folder):\n",
    "    # Initialize the video capture object\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    count = 0\n",
    "    success = True\n",
    "\n",
    "    while success:\n",
    "        # Read each new frame\n",
    "        success, img = cap.read()\n",
    "\n",
    "        # Check if the read was unsuccessful\n",
    "        if not success:\n",
    "            print(\"End of video reached.\")\n",
    "            break\n",
    "\n",
    "        # Save the current frame as a JPEG image\n",
    "        output_file = os.path.join(output_folder, f\"frame_{count}.jpg\")\n",
    "        cv2.imwrite(output_file, img)\n",
    "\n",
    "        count += 1\n",
    "\n",
    "    # Release the video capture object and close all windows\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 'path_to_video' with your actual path to video file\n",
    "# input_folder_path = \"/Users/alexis/Library/CloudStorage/OneDrive-Balayre&Co/Cranfield/Thesis/data/official_dataset/test\"\n",
    "# output_folder_path = \"/Users/alexis/Library/CloudStorage/OneDrive-Balayre&Co/Cranfield/Thesis/thesis-github-repository/data/frames\"\n",
    "# for video in os.listdir(input_folder_path):\n",
    "#     video_path = os.path.join(input_folder_path, video)\n",
    "#     file_name = video.split(\".\")[0]\n",
    "#     output_path = os.path.join(output_folder_path, file_name)\n",
    "#     os.makedirs(output_path, exist_ok=True)\n",
    "#     extract_frames(video_path, output_path)\n",
    "#     print(f\"Frames extracted from {video_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Studio Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "\n",
    "def convert_json(input_file, output_file):\n",
    "    with open(input_file, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    output_data = []\n",
    "\n",
    "    for item in data:\n",
    "        frame = item[\"file_upload\"].split(\"-\")[-1].split(\".\")[0]\n",
    "        if item[\"annotations\"]:\n",
    "            first_annotation = item[\"annotations\"][0]\n",
    "            if first_annotation[\"result\"]:\n",
    "                first_result = first_annotation[\"result\"][0]\n",
    "                bbox = first_result[\"value\"]\n",
    "\n",
    "                # Convert percentages to pixel values based on original image width/height\n",
    "                x = bbox[\"x\"] * first_result[\"original_width\"] / 100.0\n",
    "                y = bbox[\"y\"] * first_result[\"original_height\"] / 100.0\n",
    "                width = bbox[\"width\"] * first_result[\"original_width\"] / 100.0\n",
    "                height = bbox[\"height\"] * first_result[\"original_height\"] / 100.0\n",
    "\n",
    "                bbox_xywh = [x, y, width, height]\n",
    "\n",
    "                output_entry = {\n",
    "                    \"frame\": frame,\n",
    "                    \"bbox\": bbox_xywh\n",
    "                }\n",
    "\n",
    "                output_data.append(output_entry)\n",
    "\n",
    "    with open(output_file, \"w\") as f:\n",
    "        json.dump(output_data, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify your input and output file paths\n",
    "# input_file = \"/Users/alexis/Library/CloudStorage/OneDrive-Balayre&Co/Cranfield/Thesis/thesis-github-repository/code/LSTM/data/outdoor2/test_outdoor2.json\"\n",
    "# output_file = \"/Users/alexis/Library/CloudStorage/OneDrive-Balayre&Co/Cranfield/Thesis/thesis-github-repository/code/LSTM/data/outdoor2/outdoor2_full_dataset.json\"\n",
    "\n",
    "# convert_json(input_file, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthetic Data Generation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to generate a Background Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def generate_background_image(width, height, num_shapes, shape_size):\n",
    "    \"\"\"\n",
    "    Generate a simple background image with random shapes.\n",
    "\n",
    "    Parameters:\n",
    "    width (int): Width of the background image\n",
    "    height (int): Height of the background image\n",
    "    num_shapes (int): Number of shapes to draw\n",
    "    shape_size (int): Size of each shape\n",
    "\n",
    "    Returns:\n",
    "    img (numpy array): Background image\n",
    "    \"\"\"\n",
    "    img = np.zeros((height, width, 3), dtype=np.uint8)  # Initialize black background\n",
    "\n",
    "    for _ in range(num_shapes):\n",
    "        x = np.random.randint(0, width - shape_size)\n",
    "        y = np.random.randint(0, height - shape_size)\n",
    "        cv2.rectangle(\n",
    "            img,\n",
    "            (x, y),\n",
    "            (x + shape_size, y + shape_size),\n",
    "            (\n",
    "                np.random.randint(0, 255),\n",
    "                np.random.randint(0, 255),\n",
    "                np.random.randint(0, 255),\n",
    "            ),\n",
    "            -1,\n",
    "        )\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "# width, height = 640, 480  # Dimensions of the background image\n",
    "# num_shapes = 10  # Number of shapes to draw\n",
    "# shape_size = 20  # Size of each shape\n",
    "# background_image = generate_background_image(\n",
    "#     width, height, num_shapes, shape_size\n",
    "# )  # Generate background image\n",
    "# cv2.imwrite(\n",
    "#     \"/Users/alexis/Library/CloudStorage/OneDrive-Balayre&Co/Cranfield/Thesis/thesis-github-repository/data/synthetic/background/background1.png\",\n",
    "#     background_image,\n",
    "# )  # Save background image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Camera intrinsic matrix\n",
    "\n",
    "1. Setting Up the Camera Projection\n",
    "   First, define the parameters for the camera:\n",
    "\n",
    "**Intrinsic matrix (K)**: Describes the camera's internal parameters (focal length, optical center).  \n",
    "**Extrinsic matrix (R, T)**: Describes the camera's position and orientation in the world (rotation and translation).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = np.array([[1000, 0, 128], [0, 1000, 128], [0, 0, 1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to project 3D points to 2D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to project 3D points to 2D\n",
    "def project_points(points, K, R, T):\n",
    "    \"\"\"\n",
    "    Project 3D points to 2D using the pinhole camera model.\n",
    "\n",
    "    Parameters:\n",
    "        points: Nx3 array of 3D points\n",
    "        K: Intrinsic matrix\n",
    "        R: Rotation matrix\n",
    "        T: Translation vector\n",
    "    \"\"\"\n",
    "    points_homogeneous = np.hstack((points, np.ones((points.shape[0], 1))))\n",
    "    projection_matrix = K @ np.hstack((R, T.reshape(-1, 1)))\n",
    "    points_2d_homogeneous = points_homogeneous @ projection_matrix.T\n",
    "    points_2d = points_2d_homogeneous[:, :2] / points_2d_homogeneous[:, 2, np.newaxis]\n",
    "    return points_2d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to Generate 3D Linear Motion Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "\n",
    "\n",
    "# Function to generate 3D linear motion dataset\n",
    "def generate_3d_linear_motion_dataset(num_frames, bg_img, output_dir):\n",
    "    \"\"\"\n",
    "    Generate a synthetic dataset with 3D linear motion.\n",
    "\n",
    "    Parameters:\n",
    "        num_frames (int): Number of frames to generate\n",
    "        bg_img (str): Path to the background image\n",
    "        output_dir (str): Output directory to save the frames and annotations\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Rotation (identity matrix, no rotation for simplicity)\n",
    "    R = np.eye(3)\n",
    "    T_initial = np.array(\n",
    "        [0, 0, 1000]\n",
    "    )  # Initial translation (starting far from the camera)\n",
    "\n",
    "    # Initialise the list to store the annotations\n",
    "    annotations = []\n",
    "\n",
    "    for frame_num in range(num_frames):\n",
    "        T = T_initial + np.array([frame_num * 5, 0, -frame_num * 5])\n",
    "\n",
    "        # Define the 3D position of the object (can be a simple representation)\n",
    "        obj_3d_position = np.array(\n",
    "            [[0, 0, 0]]\n",
    "        )  # Object centered at origin in its local 3D coordinate space\n",
    "\n",
    "        # Project the 3D points to 2D\n",
    "        obj_2d_position = project_points(obj_3d_position, K, R, T).astype(int)[0]\n",
    "\n",
    "        # Calculate 2D bounding box\n",
    "        w, h = 50, 50\n",
    "        bbox = (obj_2d_position[0] - w // 2, obj_2d_position[1] - h // 2, w, h)\n",
    "\n",
    "        # Draw the bounding box on the background\n",
    "        img = cv2.imread(bg_img)\n",
    "        cv2.rectangle(\n",
    "            img,\n",
    "            (bbox[0], bbox[1]),\n",
    "            (bbox[0] + bbox[2], bbox[1] + bbox[3]),\n",
    "            (255, 0, 0),\n",
    "            2,\n",
    "        )\n",
    "\n",
    "        # Save the frame\n",
    "        frame_filename = f\"frame_{frame_num:04d}.png\"\n",
    "        frame_filepath = os.path.join(output_dir, frame_filename)\n",
    "        cv2.imwrite(frame_filepath, img)\n",
    "\n",
    "        # Append annotation\n",
    "        annotations.append(\n",
    "            {\"frame\": frame_filename, \"bbox\": [int(coord) for coord in bbox]}\n",
    "        )\n",
    "\n",
    "    # Save the annotations to a JSON file\n",
    "    annotations_filepath = os.path.join(output_dir, \"annotations.json\")\n",
    "    with open(annotations_filepath, \"w\") as f:\n",
    "        json.dump(annotations, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "# bg_img_path = \"/Users/alexis/Library/CloudStorage/OneDrive-Balayre&Co/Cranfield/Thesis/thesis-github-repository/data/synthetic/background/background1.png\"\n",
    "# generate_3d_linear_motion_dataset(\n",
    "#     num_frames=100,\n",
    "#     bg_img=bg_img_path,\n",
    "#     output_dir=\"/Users/alexis/Library/CloudStorage/OneDrive-Balayre&Co/Cranfield/Thesis/thesis-github-repository/data/synthetic/3d_linear_motion_dataset_test1\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to Generate 3D Non-linear Motion Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_frame(\n",
    "    frame_num,\n",
    "    T_initial,\n",
    "    base_size,\n",
    "    bg_img,\n",
    "    output_dir,\n",
    "    z_min,\n",
    "    z_max,\n",
    "    frame_size,\n",
    "    scale_factor,\n",
    "):\n",
    "    \"\"\"\n",
    "    Process a single frame by projecting 3D points into 2D and saving the image with a bounding box.\n",
    "\n",
    "    Paramètres :\n",
    "        frame_num: Frame number\n",
    "        T_initial: Initial translation vector\n",
    "        base_size: Base size of the bounding box\n",
    "        bg_img: Path to background image\n",
    "        output_dir: Directory for saving frames\n",
    "        z_min: Minimum value authorised for the camera's z coordinate\n",
    "        z_max: Maximum authorised value for the camera's z coordinate\n",
    "        frame_size : Size of the video frame\n",
    "        scale_factor: Scaling factor to slow down movement\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Rotation (identity matrix, no rotation for simplicity)\n",
    "        R = np.eye(3)\n",
    "\n",
    "        # Non-linear translation \n",
    "        t = frame_num * scale_factor\n",
    "        \"\"\" T = T_initial + np.array(\n",
    "            [100 * np.cos(t), 100 * np.sin(t), -frame_num * 3 * scale_factor]\n",
    "        ) \"\"\"\n",
    "        T = T_initial + np.array(\n",
    "            [\n",
    "                100 * np.cos(t) + 50 * np.sin(2 * t),\n",
    "                100 * np.sin(t) + 50 * np.cos(2 * t),\n",
    "                -frame_num * 3 * scale_factor,\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Ensures that the Z coordinate is within a valid range\n",
    "        T[2] = np.clip(T[2], z_min, z_max)\n",
    "\n",
    "        # Defines the object's 3D position\n",
    "        obj_3d_position = np.array([[0, 0, 0]])\n",
    "\n",
    "        # Projecting 3D points in 2D\n",
    "        obj_2d_position = project_points(obj_3d_position, K, R, T).astype(int)[0]\n",
    "\n",
    "        # Keeps the object in the frame\n",
    "        obj_2d_position[0] = np.clip(\n",
    "            obj_2d_position[0], base_size // 2, frame_size[0] - base_size // 2\n",
    "        )\n",
    "        obj_2d_position[1] = np.clip(\n",
    "            obj_2d_position[1], base_size // 2, frame_size[1] - base_size // 2\n",
    "        )\n",
    "\n",
    "        # Calculates the size of the 2D bounding box as a function of camera distance\n",
    "        z = T[2]  # Z coordinate of the camera translation vector\n",
    "        size_scaling_factor = base_size / (z / 1000)\n",
    "        w = h = max(\n",
    "            1, int(size_scaling_factor)\n",
    "        )  # Ensures that the width and height are at least 1\n",
    "\n",
    "        # Calculates the 2D bounding box\n",
    "        bbox = (obj_2d_position[0] - w // 2, obj_2d_position[1] - h // 2, w, h)\n",
    "\n",
    "        # Draw the bounding box on the background\n",
    "        img = cv2.imread(bg_img)\n",
    "        if img is None:\n",
    "            raise FileNotFoundError(f\"Image de fond {bg_img} non trouvée\")\n",
    "        cv2.rectangle(\n",
    "            img,\n",
    "            (bbox[0], bbox[1]),\n",
    "            (bbox[0] + bbox[2], bbox[1] + bbox[3]),\n",
    "            (255, 0, 0),\n",
    "            2,\n",
    "        )\n",
    "\n",
    "        # Saves the frame\n",
    "        frame_filename = f\"frame_{frame_num:04d}.png\"\n",
    "        frame_filepath = os.path.join(output_dir, frame_filename)\n",
    "        cv2.imwrite(frame_filepath, img)\n",
    "\n",
    "        return {\"frame\": frame_filename, \"bbox\": [int(coord) for coord in bbox]}\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors du traitement de la frame {frame_num} : {e}\")\n",
    "        return {\"frame\": None, \"bbox\": []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_3d_nonlinear_motion_dataset(\n",
    "    num_frames,\n",
    "    bg_img,\n",
    "    output_dir,\n",
    "    z_min=100,\n",
    "    z_max=1000,\n",
    "    frame_size=(640, 480),\n",
    "    scale_factor=0.01,\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate a synthetic dataset with 3D non-linear motion.\n",
    "\n",
    "    Paramètres:\n",
    "        num_frames (int): Frames number\n",
    "        bg_img (str): Path to background image\n",
    "        output_dir (str): Path to output folder\n",
    "        z_min (int, optional): Minimum value authorised for the camera's z coordinate. Defaults to 100.\n",
    "        z_max (int, optional): Maximum authorised value for the camera's z coordinate. Defaults to 1000.\n",
    "        frame_size (tuple, optional): Size of the video frame. Defaults to (640, 480).\n",
    "        scale_factor (float, optional): Scaling factor to slow down movement. Defaults to 0.01.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    #  Initial translation (initial distance from the camera)\n",
    "    T_initial = np.array([0, 0, z_max])\n",
    "    base_size = 50  # Basic size of the bounding box when z = z_max\n",
    "\n",
    "    # Processing each frame\n",
    "    annotations = []\n",
    "    for frame_num in range(num_frames):\n",
    "        annotation = process_frame(\n",
    "            frame_num,\n",
    "            T_initial,\n",
    "            base_size,\n",
    "            bg_img,\n",
    "            output_dir,\n",
    "            z_min,\n",
    "            z_max,\n",
    "            frame_size,\n",
    "            scale_factor,\n",
    "        )\n",
    "        if annotation[\"frame\"]:\n",
    "            annotations.append(annotation)\n",
    "\n",
    "    # Save the annotations to a JSON file\n",
    "    annotations_filepath = os.path.join(output_dir, \"annotations.json\")\n",
    "    with open(annotations_filepath, \"w\") as f:\n",
    "        json.dump(annotations, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "#     bg_img_path = \"/Users/alexis/Library/CloudStorage/OneDrive-Balayre&Co/Cranfield/Thesis/thesis-github-repository/data/synthetic/background/background1.png\"\n",
    "#     generate_3d_nonlinear_motion_dataset(\n",
    "#         num_frames=10000,\n",
    "#         bg_img=bg_img_path,\n",
    "#         output_dir=\"/Users/alexis/Library/CloudStorage/OneDrive-Balayre&Co/Cranfield/Thesis/thesis-github-repository/data/synthetic/3d_nonlinear_motion_dataset_test6\",\n",
    "#     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to create training, validation & test datasets with a classic JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def create_datasets(\n",
    "    annotation_file, train_ratio=0.64, val_ratio=0.16, test_ratio=0.2, output_dir=\".\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Creates train.json, validation.json and test.json files from annotations.json.\n",
    "\n",
    "    Parameters :\n",
    "        annotation_file (str): Path to the annotations.json file\n",
    "        train_ratio (int): Ratio of data allocated to training\n",
    "        val_ratio (int): Ratio of data assigned to validation\n",
    "        test_ratio (int) : Ratio of data allocated to tests\n",
    "        output_dir (str): Directory where the resultant files will be saved\n",
    "    \"\"\"\n",
    "    # Loads the annotations json file\n",
    "    with open(annotation_file, \"r\") as f:\n",
    "        annotations = json.load(f)\n",
    "\n",
    "    # Checks that the division ratios are correct\n",
    "    assert train_ratio + val_ratio + test_ratio == 1.0, \"Ratios should slumber at 1.0\"\n",
    "\n",
    "    num_annotations = len(annotations)\n",
    "\n",
    "    # Calculer les indices de division\n",
    "    train_end = int(train_ratio * num_annotations)\n",
    "    val_end = int((train_ratio + val_ratio) * num_annotations)\n",
    "\n",
    "    # Découper les annotations en chaînes continues pour l'entraînement, la validation et les tests\n",
    "    train_annotations = annotations[:train_end]\n",
    "    val_annotations = annotations[train_end:val_end]\n",
    "    test_annotations = annotations[val_end:]\n",
    "\n",
    "    # Saves JSON files for training, validation and testing\n",
    "    with open(f\"{output_dir}/train.json\", \"w\") as f:\n",
    "        json.dump(train_annotations, f, indent=4)\n",
    "\n",
    "    with open(f\"{output_dir}/validation.json\", \"w\") as f:\n",
    "        json.dump(val_annotations, f, indent=4)\n",
    "\n",
    "    with open(f\"{output_dir}/test.json\", \"w\") as f:\n",
    "        json.dump(test_annotations, f, indent=4)\n",
    "\n",
    "    print(\n",
    "        \"‘The files train.json, validation.json and test.json have been created and saved.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‘The files train.json, validation.json and test.json have been created and saved.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "create_datasets(\n",
    "    \"/Users/alexis/Library/CloudStorage/OneDrive-Balayre&Co/Cranfield/Thesis/thesis-github-repository/code/LSTM/data/outdoor1/outdoor1_full_dataset.json\",\n",
    "    output_dir=\"/Users/alexis/Library/CloudStorage/OneDrive-Balayre&Co/Cranfield/Thesis/thesis-github-repository/code/LSTM/data/outdoor1\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to create training, validation & test datasets with a YOLO format dataset folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def create_splits_and_yaml(dataset_path, train_size=0.64, val_size=0.16, test_size=0.2):\n",
    "    dataset_path = Path(dataset_path)\n",
    "    images_path = dataset_path / \"images\"\n",
    "    labels_path = dataset_path / \"labels\"\n",
    "\n",
    "    assert images_path.exists() and labels_path.exists(), \"Invalid dataset path, or missing 'images' and 'labels' folders.\"\n",
    "\n",
    "    # Get all image files\n",
    "    images = list(images_path.glob(\"*.jpg\")) + list(images_path.glob(\"*.png\"))  # depending on your dataset format\n",
    "\n",
    "    # Split the dataset\n",
    "    train_images, test_images = train_test_split(images, test_size=test_size, random_state=42)\n",
    "    train_images, val_images = train_test_split(train_images, test_size=val_size/(train_size+val_size), random_state=42)\n",
    "\n",
    "    def move_files(file_list, dest_images_folder, dest_labels_folder):\n",
    "        for image in file_list:\n",
    "            label_file = labels_path / (image.stem + \".txt\")\n",
    "            if label_file.exists():\n",
    "                shutil.copy(label_file, dest_labels_folder / label_file.name)\n",
    "            shutil.copy(image, dest_images_folder / image.name)\n",
    "\n",
    "    split_folders = [\"train\", \"val\", \"test\"]\n",
    "    for folder in split_folders:\n",
    "        dest_images_folder = dataset_path / folder / \"images\"\n",
    "        dest_labels_folder = dataset_path / folder / \"labels\"\n",
    "        (dataset_path / folder).mkdir(parents=True, exist_ok=True)\n",
    "        dest_images_folder.mkdir(exist_ok=True)\n",
    "        dest_labels_folder.mkdir(exist_ok=True)\n",
    "\n",
    "    move_files(train_images, dataset_path / \"train\" / \"images\", dataset_path / \"train\" / \"labels\")\n",
    "    move_files(val_images, dataset_path / \"val\" / \"images\", dataset_path / \"val\" / \"labels\")\n",
    "    move_files(test_images, dataset_path / \"test\" / \"images\", dataset_path / \"test\" / \"labels\")\n",
    "\n",
    "    # Create the .yaml file\n",
    "    yaml_content = f\"\"\"\n",
    "    train: {str(dataset_path / 'train')}\n",
    "    val: {str(dataset_path / 'val')}\n",
    "    test: {str(dataset_path / 'test')}\n",
    "\n",
    "    # number of classes\n",
    "    nc: 1\n",
    "\n",
    "    # class names\n",
    "    names: ['fuel_port']\n",
    "    \"\"\"\n",
    "\n",
    "    with open(dataset_path / \"dataset.yaml\", \"w\") as yaml_file:\n",
    "        yaml_file.write(yaml_content.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "dataset_folder = \"/Users/alexis/Library/CloudStorage/OneDrive-Balayre&Co/Cranfield/Thesis/thesis-github-repository/data/frames/YOLO\"  # replace with your actual dataset path\n",
    "create_splits_and_yaml(dataset_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converter for the provided dataset to YOLO format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from glob import glob\n",
    "\n",
    "def convert_annotation(json_file, output_dir):\n",
    "    with open(json_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    image_path = os.path.join(output_dir, os.path.splitext(os.path.basename(json_file))[0] + '.jpg')\n",
    "    txt_output_path = os.path.join(output_dir, os.path.splitext(os.path.basename(json_file))[0] + '.txt')\n",
    "\n",
    "    with open(txt_output_path, 'w') as txt_out:\n",
    "        for shape in data['shapes']:\n",
    "            if shape['shape_type'] != 'rectangle':\n",
    "                continue\n",
    "\n",
    "            label = shape['label']\n",
    "            points = shape['points']\n",
    "            x1, y1 = points[0]\n",
    "            x2, y2 = points[1]\n",
    "\n",
    "            # Convert to YOLO format\n",
    "            width = data['imageWidth']\n",
    "            height = data['imageHeight']\n",
    "            xc = (x1 + x2) / 2 / width\n",
    "            yc = (y1 + y2) / 2 / height\n",
    "            w = (x2 - x1) / width\n",
    "            h = (y2 - y1) / height\n",
    "\n",
    "            class_id = 0  # Update this if you have multiple classes and a class mapping system\n",
    "            txt_out.write(f\"{class_id} {xc} {yc} {w} {h}\\n\")\n",
    "\n",
    "def convert_annotations_in_directory(input_dir, output_dir):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    json_files = glob(os.path.join(input_dir, '*.json'))\n",
    "\n",
    "    for json_file in json_files:\n",
    "        convert_annotation(json_file, output_dir)\n",
    "\n",
    "    # Ensure image files are also copied over to the output directory\n",
    "    image_files = glob(os.path.join(input_dir, '*.jpg'))\n",
    "    for image_file in image_files:\n",
    "        dest_file = os.path.join(output_dir, os.path.basename(image_file))\n",
    "        if not os.path.exists(dest_file):\n",
    "            os.symlink(image_file, dest_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    input_directory = 'input_dir'  # Replace with the path to your input directory\n",
    "    output_directory = 'output_dir'  # Replace with the path to your output directory\n",
    "\n",
    "    convert_annotations_in_directory(input_directory, output_directory)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-cranfield-thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
