{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3dd723e5-b785-4876-8a41-7fbc1b395680",
   "metadata": {},
   "source": [
    "## Install Dependencies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82da47fb-01c7-4e37-bcc3-c912841332c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q git+https://github.com/THU-MIG/yolov10.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24e94ae-319f-428e-84d1-c7ed8aa7413e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install huggingface-hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3796aab1-524a-4fb6-87cc-19fc8bb110cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "HOME = os.getcwd()\n",
    "print(HOME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a7e9a0-424a-4a64-86c0-f61a19c784ba",
   "metadata": {},
   "source": [
    "## Install pretrained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45de0004-f0cf-42b7-955a-f40f29684197",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p {HOME}/weights\n",
    "!wget -P {HOME}/weights -q https://github.com/THU-MIG/yolov10/releases/download/v1.1/yolov10s.pt\n",
    "!wget -P {HOME}/weights -q https://github.com/THU-MIG/yolov10/releases/download/v1.1/yolov10m.pt\n",
    "!wget -P {HOME}/weights -q https://github.com/THU-MIG/yolov10/releases/download/v1.1/yolov10b.pt\n",
    "!ls -lh {HOME}/weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1fa631f-a970-4fb4-83c7-48d04040c7a9",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee4af19-e9f1-4623-b5b1-e5b6cf8a586c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLOv10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07febeb-b504-406a-9fb6-d76837ab6059",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLOv10(\"/Users/alexis/Programmation/label-studio-ml-backend/yolo_backend/best.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c413f985-ec4e-4b41-a187-16ff09263548",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(data='dataset.yaml', epochs=500, batch=16, imgsz=640)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ce14b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform object detection on an image\n",
    "results = model(\"/teamspace/studios/this_studio/test/images/0a4299fb-frame_257.jpg\")\n",
    "\n",
    "# Display the results\n",
    "results[0].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948fd380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform object detection on an image\n",
    "results = model(\"/teamspace/studios/this_studio/test/images/fafb4c4e-frame_40.jpg\")\n",
    "\n",
    "# Display the results\n",
    "results[0].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9251eef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform object detection on an image\n",
    "results = model(\"/teamspace/studios/this_studio/data_test/frame_8.jpg\")\n",
    "\n",
    "# Display the results\n",
    "results[0].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c810c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform object detection on an image\n",
    "results = model(\"/teamspace/studios/this_studio/data_test/frame_72.jpg\")\n",
    "\n",
    "# Display the results\n",
    "results[0].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2bb44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform object detection on an image\n",
    "results = model(\"/teamspace/studios/this_studio/data_test/frame_116.jpg\")\n",
    "\n",
    "# Display the results\n",
    "results[0].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91df5787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform object detection on an image\n",
    "results = model(\"/teamspace/studios/this_studio/data_test/frame_323.jpg\")\n",
    "\n",
    "# Display the results\n",
    "results[0].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e34ecd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform object detection on an image\n",
    "results = model(\"/teamspace/studios/this_studio/data_test/frame_433.jpg\")\n",
    "\n",
    "# Display the results\n",
    "results[0].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e429a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform object detection on an image\n",
    "results = model(\"/teamspace/studios/this_studio/data_test/image.png\")\n",
    "\n",
    "# Display the results\n",
    "results[0].show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581e3ca9",
   "metadata": {},
   "source": [
    "# Model Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc3631a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code took from the ultralytics repository\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from ultralytics import YOLOv10\n",
    "\n",
    "from ultralytics.utils.checks import check_imshow\n",
    "from ultralytics.utils.plotting import Annotator, colors\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "track_history = defaultdict(lambda: [])\n",
    "model = YOLOv10(\"/Users/alexis/Programmation/label-studio-ml-backend/yolo_backend/best.pt\")\n",
    "names = model.model.names\n",
    "\n",
    "video_path = \"/Users/alexis/Library/CloudStorage/OneDrive-Balayre&Co/Cranfield/Thesis/thesis-github-repository/code/object_detection/YOLOv10/video_lab_semiopen_1______3.avi\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "assert cap.isOpened(), \"Error reading video file\"\n",
    "\n",
    "w, h, fps = (int(cap.get(x)) for x in (cv2.CAP_PROP_FRAME_WIDTH, cv2.CAP_PROP_FRAME_HEIGHT, cv2.CAP_PROP_FPS))\n",
    "\n",
    "result = cv2.VideoWriter(\"video_lab_semiopen_1______3_object_tracking.avi\",\n",
    "                       cv2.VideoWriter_fourcc(*'mp4v'),\n",
    "                       fps,\n",
    "                       (w, h))\n",
    "\n",
    "while cap.isOpened():\n",
    "    success, frame = cap.read()\n",
    "    if success:\n",
    "        results = model.track(frame, persist=True, verbose=False)\n",
    "        boxes = results[0].boxes.xyxy.cpu()\n",
    "\n",
    "        if results[0].boxes.id is not None:\n",
    "\n",
    "            # Extract prediction results\n",
    "            clss = results[0].boxes.cls.cpu().tolist()\n",
    "            track_ids = results[0].boxes.id.int().cpu().tolist()\n",
    "            confs = results[0].boxes.conf.float().cpu().tolist()\n",
    "\n",
    "            # Annotator Init\n",
    "            annotator = Annotator(frame, line_width=2)\n",
    "\n",
    "            for box, cls, track_id in zip(boxes, clss, track_ids):\n",
    "                annotator.box_label(box, color=colors(int(cls), True), label=names[int(cls)])\n",
    "\n",
    "                # Store tracking history\n",
    "                track = track_history[track_id]\n",
    "                track.append((int((box[0] + box[2]) / 2), int((box[1] + box[3]) / 2)))\n",
    "                if len(track) > 30:\n",
    "                    track.pop(0)\n",
    "\n",
    "                # Plot tracks\n",
    "                #points = np.array(track, dtype=np.int32).reshape((-1, 1, 2))\n",
    "                #cv2.circle(frame, (track[-1]), 7, colors(int(cls), True), -1)\n",
    "                #cv2.polylines(frame, [points], isClosed=False, color=colors(int(cls), True), thickness=2)\n",
    "\n",
    "        result.write(frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "\n",
    "result.release()\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0760a7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.1.34 ðŸš€ Python-3.11.7 torch-2.3.0 CPU (Apple M1 Pro)\n",
      "YOLOv10m summary (fused): 369 layers, 16451542 parameters, 0 gradients, 63.4 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/alexis/Library/CloudStorage/OneDrive-Balayre&Co/Cranfield/Thesis/thesis-github-repository/data/frames/full_dataset_annotated_YOLO/test/labels.cache... 2208 images, 53 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2208/2208 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 10/18 [10:41<08:46, 65.75s/it]"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLOv10\n",
    "\n",
    "model = YOLOv10(\"/Users/alexis/Programmation/label-studio-ml-backend/yolo_backend/best.pt\")\n",
    "model.val(data='/Users/alexis/Library/CloudStorage/OneDrive-Balayre&Co/Cranfield/Thesis/thesis-github-repository/data/frames/full_dataset_annotated_YOLO/dataset.yaml', batch=128)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
