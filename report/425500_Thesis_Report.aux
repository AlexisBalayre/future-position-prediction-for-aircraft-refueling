\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\providecommand \oddpage@label [2]{}
\@writefile{toc}{\contentsline {chapter}{Academic Integrity Declaration}{i}{Doc-Start}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{Abstract}{ii}{chapter*.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {chapter}{Acknowledgements}{iii}{chapter*.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {chapter}{Table of Contents}{iv}{chapter*.3}\protected@file@percent }
\citation{ImageRefueling}
\citation{AAREKF}
\citation{AGRPoseEstimation}
\citation{DatasetAGR}
\citation{huggingface2023objectdetection}
\citation{SurveyDLOD}
\citation{LearnOpenCVYOLOv10}
\citation{wang2024yolov10}
\citation{LearnOpenCVYOLOv10}
\citation{huggingface2023objectdetection}
\citation{MultipleObjectForecasting}
\citation{DBLP:journals/corr/abs-2010-10270}
\citation{FusionGRU}
\@writefile{toc}{\contentsline {chapter}{List of Figures}{vi}{chapter*.4}\protected@file@percent }
\citation{wang2024yolov10}
\@writefile{toc}{\contentsline {chapter}{List of Tables}{viii}{chapter*.5}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{List of Abbreviations}{x}{chapter*.6}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\citation{blakey2011aviation}
\citation{sati2019aircraft}
\citation{blakey2011aviation}
\citation{doi:10.1080/13669877.2013.879493,CostsOfUnsafetyAviation}
\citation{ImageRefueling}
\citation{ImageRefueling}
\citation{10.1007/978-981-16-5943-0_26}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{1}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Background and Motivation}{1}{section.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces Pressure Refuelling of a Commercial Aircraft. Photo Credit: Tom Boon/Simple Flying\nobreakspace  {}\cite  {ImageRefueling}\relax }}{1}{figure.caption.7}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:pressure-refuelling}{{1.1}{1}{Pressure Refuelling of a Commercial Aircraft. Photo Credit: Tom Boon/Simple Flying~\cite {ImageRefueling}\relax }{figure.caption.7}{}}
\newlabel{fig:motion-blur-example}{{1.2a}{2}{Motion Blur Example\relax }{figure.caption.8}{}}
\newlabel{sub@fig:motion-blur-example}{{a}{2}{Motion Blur Example\relax }{figure.caption.8}{}}
\newlabel{fig:occlusion-example}{{1.2b}{2}{Occlusion Example\relax }{figure.caption.8}{}}
\newlabel{sub@fig:occlusion-example}{{b}{2}{Occlusion Example\relax }{figure.caption.8}{}}
\newlabel{fig:out-of-view-example}{{1.2c}{2}{Out-of-View Example\relax }{figure.caption.8}{}}
\newlabel{sub@fig:out-of-view-example}{{c}{2}{Out-of-View Example\relax }{figure.caption.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.2}{\ignorespaces Challenges in Detecting Aircraft Refuelling Port\relax }}{2}{figure.caption.8}\protected@file@percent }
\newlabel{fig:challenges-detecting-ports}{{1.2}{2}{Challenges in Detecting Aircraft Refuelling Port\relax }{figure.caption.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Research Gap}{2}{section.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Aim and Objectives}{3}{section.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.4}Technological Contributions}{3}{section.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.5}Thesis Layout}{3}{section.1.5}\protected@file@percent }
\citation{ExpertSystemsAGR}
\citation{Schultz1986}
\citation{Bennett1991}
\citation{Burnette2010}
\citation{Ficken2017}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Literature Review}{4}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:literature-review}{{2}{4}{Literature Review}{chapter.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Automated Refulling Systems in the Aviation Industry}{4}{section.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces AFRL's Automated Aircraft Ground Refueling system prototype robot (Photo Credit: AFRL/RXQ Robotics Group)\relax }}{4}{figure.caption.9}\protected@file@percent }
\newlabel{fig:test-2010}{{2.1}{4}{AFRL's Automated Aircraft Ground Refueling system prototype robot (Photo Credit: AFRL/RXQ Robotics Group)\relax }{figure.caption.9}{}}
\newlabel{fig:test-2010}{{2.1}{4}{AFRL's Automated Aircraft Ground Refueling system prototype robot (Photo Credit: AFRL/RXQ Robotics Group)\relax }{figure.caption.9}{}}
\citation{AR3P2020}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces AR3P Concept Development Prototype Robot (Photo Credit: U.S. Army)\relax }}{5}{figure.caption.10}\protected@file@percent }
\newlabel{fig:test-2017}{{2.2}{5}{AR3P Concept Development Prototype Robot (Photo Credit: U.S. Army)\relax }{figure.caption.10}{}}
\newlabel{test-2020-2}{{2.3a}{5}{AR3P Robot Approaching Detected Aircraft (Photo Credit: Stratom)\relax }{figure.caption.11}{}}
\newlabel{sub@test-2020-2}{{a}{5}{AR3P Robot Approaching Detected Aircraft (Photo Credit: Stratom)\relax }{figure.caption.11}{}}
\newlabel{test-2020-3}{{2.3b}{5}{AR3P Robot Engaging Aircraft Refueling Port (Photo Credit: Stratom)\relax }{figure.caption.11}{}}
\newlabel{sub@test-2020-3}{{b}{5}{AR3P Robot Engaging Aircraft Refueling Port (Photo Credit: Stratom)\relax }{figure.caption.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces AR3P Robot Hot Refueling Demonstration for S-70 Helicopter\relax }}{5}{figure.caption.11}\protected@file@percent }
\newlabel{fig:automated-refuelling-systems}{{2.3}{5}{AR3P Robot Hot Refueling Demonstration for S-70 Helicopter\relax }{figure.caption.11}{}}
\citation{AARBinocularVision,AARCNN}
\citation{AAREKF}
\citation{AAREKF}
\citation{AAREKF}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Autonomous Aerial Refueling (AAR) of X-47B Unmanned Combat Air System Demonstrator (Photo Credit: U.S. Navy)\relax }}{6}{figure.caption.12}\protected@file@percent }
\newlabel{fig:aerial-refuelling}{{2.4}{6}{Autonomous Aerial Refueling (AAR) of X-47B Unmanned Combat Air System Demonstrator (Photo Credit: U.S. Navy)\relax }{figure.caption.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces Autonomous Air Refueling Detection System with EKF. Source: \citet  {AAREKF}\relax }}{6}{figure.caption.13}\protected@file@percent }
\newlabel{fig:detection-system-aarekf}{{2.5}{6}{Autonomous Air Refueling Detection System with EKF. Source: \citet {AAREKF}\relax }{figure.caption.13}{}}
\citation{AGRPoseEstimation}
\citation{AGRPoseEstimation}
\citation{AGRPoseEstimation}
\citation{DatasetAGR}
\citation{HybridDatasetAGRV1}
\citation{DatasetAGR}
\citation{DatasetAGR}
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces Kalman Filter Workflow for Pose Estimation in Autonomous Ground Refueling. Source: \citet  {AGRPoseEstimation}\relax }}{7}{figure.caption.14}\protected@file@percent }
\newlabel{fig:agr-pose-kalman}{{2.6}{7}{Kalman Filter Workflow for Pose Estimation in Autonomous Ground Refueling. Source: \citet {AGRPoseEstimation}\relax }{figure.caption.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces AAGR Dataset Overview. Source: \citet  {DatasetAGR}\relax }}{7}{figure.caption.15}\protected@file@percent }
\newlabel{fig:agr-dataset}{{2.7}{7}{AAGR Dataset Overview. Source: \citet {DatasetAGR}\relax }{figure.caption.15}{}}
\citation{huggingface2023objectdetection}
\citation{huggingface2023objectdetection}
\citation{huggingface2023objectdetection}
\citation{DBLP:journals/corr/GirshickDDM13}
\citation{DBLP:journals/corr/Girshick15}
\citation{Ren2017}
\citation{DBLP:journals/corr/DaiLHS16}
\citation{SurveyDLOD,ODNetworkUAVCNNTransformer}
\citation{DBLP:journals/corr/LiuAESR15}
\citation{DBLP:journals/corr/RedmonDGF15,DBLP:journals/corr/RedmonF16,DBLP:journals/corr/abs-2004-10934,chen2023yoloms,DBLP:journals/corr/abs-2107-08430,YOLOv5Release,li2023yolov6,YOLOv8,wang2024yolov9,xu2022ppyoloe,wang2023goldyolo,xu2023damoyolo,wang2024yolov10}
\citation{lin2018focal}
\citation{SurveyDLOD,ODNetworkUAVCNNTransformer}
\citation{SurveyDLOD}
\citation{SurveyDLOD}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Object Detection and Tracking in Computer Vision}{8}{section.2.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.8}{\ignorespaces Example of outputs from an object detector\nobreakspace  {}\cite  {huggingface2023objectdetection}.\relax }}{8}{figure.caption.16}\protected@file@percent }
\newlabel{fig:object-detection}{{2.8}{8}{Example of outputs from an object detector~\cite {huggingface2023objectdetection}.\relax }{figure.caption.16}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.9}{\ignorespaces Basic deep learning-based one-stage vs two-stage object detection model architectures\nobreakspace  {}\cite  {SurveyDLOD}.\relax }}{8}{figure.caption.17}\protected@file@percent }
\newlabel{fig:two-stage-vs-single-stage}{{2.9}{8}{Basic deep learning-based one-stage vs two-stage object detection model architectures~\cite {SurveyDLOD}.\relax }{figure.caption.17}{}}
\citation{LearnOpenCVYOLOv10}
\citation{LearnOpenCVYOLOv10}
\citation{LearnOpenCVYOLOv10}
\citation{wang2024yolov10}
\citation{wang2024yolov10}
\citation{wang2024yolov10}
\citation{wang2024yolov10,LearnOpenCVYOLOv10}
\citation{LearnOpenCVYOLOv10}
\citation{LearnOpenCVYOLOv10}
\@writefile{lof}{\contentsline {figure}{\numberline {2.10}{\ignorespaces Non-Maximum Suppression (NMS) in Object Detection\nobreakspace  {}\cite  {LearnOpenCVYOLOv10}.\relax }}{9}{figure.caption.18}\protected@file@percent }
\newlabel{fig:nms}{{2.10}{9}{Non-Maximum Suppression (NMS) in Object Detection~\cite {LearnOpenCVYOLOv10}.\relax }{figure.caption.18}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.11}{\ignorespaces YOLOv10 Model Workflow\nobreakspace  {}\cite  {wang2024yolov10}\relax }}{9}{figure.caption.19}\protected@file@percent }
\newlabel{fig:yolov10}{{2.11}{9}{YOLOv10 Model Workflow~\cite {wang2024yolov10}\relax }{figure.caption.19}{}}
\citation{wang2024yolov10}
\citation{wang2024yolov10}
\@writefile{lof}{\contentsline {figure}{\numberline {2.12}{\ignorespaces Large-Kernel Convolution in YOLOv10\nobreakspace  {}\cite  {LearnOpenCVYOLOv10}\relax }}{10}{figure.caption.20}\protected@file@percent }
\newlabel{fig:large-kernel-yolov10}{{2.12}{10}{Large-Kernel Convolution in YOLOv10~\cite {LearnOpenCVYOLOv10}\relax }{figure.caption.20}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2.1}{\ignorespaces Performance Comparison of YOLO Models with State-of-the-Art Techniques. Latency is reported using official pre-trained models. \textsuperscript  {f}Latency refers to the forward pass duration without including post-processing. \textsuperscript  {\textdagger } indicates YOLOv10 results obtained with the original one-to-many training and Non-Maximum Suppression (NMS)\nobreakspace  {}\cite  {wang2024yolov10}.\relax }}{10}{table.caption.21}\protected@file@percent }
\newlabel{tab:yolov10-benchmarks}{{2.1}{10}{Performance Comparison of YOLO Models with State-of-the-Art Techniques. Latency is reported using official pre-trained models. \textsuperscript {f}Latency refers to the forward pass duration without including post-processing. \textsuperscript {\textdagger } indicates YOLOv10 results obtained with the original one-to-many training and Non-Maximum Suppression (NMS)~\cite {wang2024yolov10}.\relax }{table.caption.21}{}}
\citation{huggingface2023objectdetection}
\citation{huggingface2023objectdetection}
\citation{huggingface2023objectdetection}
\citation{huggingface2023objectdetection}
\citation{huggingface2023objectdetection}
\citation{huggingface2023objectdetection}
\citation{huggingface2023objectdetection}
\citation{huggingface2023objectdetection}
\citation{SurveyVisualOT}
\citation{SurveyVisualOT}
\citation{OverviewCorrelationAlgoOT,SuveyAdvancesSingleOTMethods,SurveyModernODModels}
\citation{SuveyAdvancesSingleOTMethods,SurveyModernODModels,SurveyTransformersSingleOT}
\citation{SurveyTransformersSingleOT}
\citation{SurveySmallObjectDetection,SmallObjectDetectionPositonPrediction}
\citation{SuveyAdvancesSingleOTMethods,SurveyModernODModels}
\citation{OverviewCorrelationAlgoOT,SuveyAdvancesSingleOTMethods}
\@writefile{lof}{\contentsline {figure}{\numberline {2.13}{\ignorespaces Intersection over Union (IoU) between a detection (in green) and ground-truth (in blue).\nobreakspace  {}\cite  {huggingface2023objectdetection}\relax }}{11}{figure.caption.22}\protected@file@percent }
\newlabel{fig:iou-metric}{{2.13}{11}{Intersection over Union (IoU) between a detection (in green) and ground-truth (in blue).~\cite {huggingface2023objectdetection}\relax }{figure.caption.22}{}}
\citation{FFPSpaceSystemVehicles}
\citation{Alahi2016}
\citation{MultipleObjectForecasting}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Deep Learning for Spacio-Temporal Prediction}{13}{section.2.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.14}{\ignorespaces Comparing different Sequence models: RNN, LSTM, and GRU. Source: Colah's blog. Compiled by AIML.com\relax }}{13}{figure.caption.23}\protected@file@percent }
\newlabel{fig:lstm-rnn-gru}{{2.14}{13}{Comparing different Sequence models: RNN, LSTM, and GRU. Source: Colah's blog. Compiled by AIML.com\relax }{figure.caption.23}{}}
\citation{MultipleObjectForecasting}
\citation{MultipleObjectForecasting}
\citation{MultipleObjectForecasting}
\citation{MultipleObjectForecasting}
\citation{MultipleObjectForecasting}
\citation{DBLP:journals/corr/abs-2010-10270}
\@writefile{lof}{\contentsline {figure}{\numberline {2.15}{\ignorespaces STED Model Architecture. Source:\nobreakspace  {}\citet  {MultipleObjectForecasting}\relax }}{14}{figure.caption.24}\protected@file@percent }
\newlabel{fig:sted}{{2.15}{14}{STED Model Architecture. Source:~\citet {MultipleObjectForecasting}\relax }{figure.caption.24}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2.2}{\ignorespaces Comparison of the performance of STED with baseline models on the Citywalks dataset. Metrics include Average Displacement Error (ADE), Final Displacement Error (FDE), Average Intersection-over-Union (AIoU), and Final Intersection-over-Union (FIoU). The model was evaluated using 1 second of input frames to predict 2 seconds of future frames.\relax }}{14}{table.caption.25}\protected@file@percent }
\newlabel{tab:sted-results}{{2.2}{14}{Comparison of the performance of STED with baseline models on the Citywalks dataset. Metrics include Average Displacement Error (ADE), Final Displacement Error (FDE), Average Intersection-over-Union (AIoU), and Final Intersection-over-Union (FIoU). The model was evaluated using 1 second of input frames to predict 2 seconds of future frames.\relax }{table.caption.25}{}}
\citation{DBLP:journals/corr/abs-2010-10270}
\citation{DBLP:journals/corr/abs-2010-10270}
\citation{DBLP:journals/corr/abs-2010-10270}
\citation{DBLP:journals/corr/abs-2010-10270}
\citation{DBLP:journals/corr/abs-2010-10270}
\citation{DBLP:journals/corr/abs-2010-10270}
\@writefile{lof}{\contentsline {figure}{\numberline {2.16}{\ignorespaces PV-LSTM Model Architecture. Source:\nobreakspace  {}\citet  {DBLP:journals/corr/abs-2010-10270}\relax }}{15}{figure.caption.26}\protected@file@percent }
\newlabel{fig:pv-lstm}{{2.16}{15}{PV-LSTM Model Architecture. Source:~\citet {DBLP:journals/corr/abs-2010-10270}\relax }{figure.caption.26}{}}
\citation{FusionGRU}
\citation{FusionGRU}
\citation{FusionGRU}
\@writefile{lot}{\contentsline {table}{\numberline {2.3}{\ignorespaces Comparison of the performance of PV-LSTM with baseline models on the Citywalks dataset. Metrics include Average Displacement Error (ADE), Final Displacement Error (FDE), and Average Intersection-over-Union (AIoU). The model was evaluated using 1 second of input frames to predict 2 seconds of future frames.\relax }}{16}{table.caption.27}\protected@file@percent }
\newlabel{tab:pv-lstm-results}{{2.3}{16}{Comparison of the performance of PV-LSTM with baseline models on the Citywalks dataset. Metrics include Average Displacement Error (ADE), Final Displacement Error (FDE), and Average Intersection-over-Union (AIoU). The model was evaluated using 1 second of input frames to predict 2 seconds of future frames.\relax }{table.caption.27}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.17}{\ignorespaces Fusion-GRU model architecture. Source:\nobreakspace  {}\citet  {FusionGRU}\relax }}{16}{figure.caption.28}\protected@file@percent }
\newlabel{fig:fusion-gru}{{2.17}{16}{Fusion-GRU model architecture. Source:~\citet {FusionGRU}\relax }{figure.caption.28}{}}
\citation{FusionGRU}
\citation{FusionGRU}
\citation{FusionGRU}
\@writefile{lot}{\contentsline {table}{\numberline {2.4}{\ignorespaces Comparison of the performance of Fusion-GRU with baseline models on the ROL and HEV-I datasets. Metrics include ADE, FDE, and FIoU. The model was evaluated using 0.5-second and 1-second prediction horizons.\relax }}{17}{table.caption.29}\protected@file@percent }
\newlabel{tab:fusion-gru-results}{{2.4}{17}{Comparison of the performance of Fusion-GRU with baseline models on the ROL and HEV-I datasets. Metrics include ADE, FDE, and FIoU. The model was evaluated using 0.5-second and 1-second prediction horizons.\relax }{table.caption.29}{}}
\citation{IntelRealSense}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Methodology}{18}{chapter.3}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:methodology}{{3}{18}{Methodology}{chapter.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Dataset Configuration}{18}{section.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}Dataset Description}{18}{subsection.3.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Intel® ${\text  {RealSense}}^{\text  {TM}}$ D435 Depth Camera. Source: Intel\relax }}{18}{figure.caption.30}\protected@file@percent }
\newlabel{fig:intel-realsense-d435}{{3.1}{18}{Intel® ${\text {RealSense}}^{\text {TM}}$ D435 Depth Camera. Source: Intel\relax }{figure.caption.30}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.2}Data Annotation}{19}{subsection.3.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.3}Summary of Available Videos}{20}{subsection.3.1.3}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {3.1}{\ignorespaces \centering  Summary of available videos in the HARD dataset with their assignment.\relax }}{20}{table.caption.31}\protected@file@percent }
\newlabel{tab:video_summary}{{3.1}{20}{\centering Summary of available videos in the HARD dataset with their assignment.\relax }{table.caption.31}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.4}Data Distribution}{21}{subsection.3.1.4}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {3.2}{\ignorespaces \centering  Distribution of frames across train, test, and validation sets for each state in the HARD dataset before balancing.\relax }}{21}{table.caption.32}\protected@file@percent }
\newlabel{tab:frame_distribution}{{3.2}{21}{\centering Distribution of frames across train, test, and validation sets for each state in the HARD dataset before balancing.\relax }{table.caption.32}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.5}Data Balancing}{21}{subsection.3.1.5}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {3.3}{\ignorespaces \centering  Distribution of frames across train, test, and validation sets for each state in the HARD dataset after balancing.\relax }}{21}{table.caption.33}\protected@file@percent }
\newlabel{tab:balanced_frame_distribution}{{3.3}{21}{\centering Distribution of frames across train, test, and validation sets for each state in the HARD dataset after balancing.\relax }{table.caption.33}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.6}Example Images from the Datase}{22}{subsection.3.1.6}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Annotated images of the refueling port in the CLOSED state.\relax }}{22}{figure.caption.34}\protected@file@percent }
\newlabel{fig:grid-closed-images}{{3.2}{22}{Annotated images of the refueling port in the CLOSED state.\relax }{figure.caption.34}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Annotated images of the refueling port in the OPEN state.\relax }}{22}{figure.caption.35}\protected@file@percent }
\newlabel{fig:grid-open-images}{{3.3}{22}{Annotated images of the refueling port in the OPEN state.\relax }{figure.caption.35}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces Annotated images of the refueling port in the SEMI-OPEN state.\relax }}{22}{figure.caption.36}\protected@file@percent }
\newlabel{fig:grid-semi-open-images}{{3.4}{22}{Annotated images of the refueling port in the SEMI-OPEN state.\relax }{figure.caption.36}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.7}Analysis of Temporal Dynamics with Savitzky-Golay Filtering}{23}{subsection.3.1.7}\protected@file@percent }
\newlabel{fig:central-position-test-indoor1}{{3.5a}{24}{Central position of the refueling port over time (Raw Data).\relax }{figure.caption.37}{}}
\newlabel{sub@fig:central-position-test-indoor1}{{a}{24}{Central position of the refueling port over time (Raw Data).\relax }{figure.caption.37}{}}
\newlabel{fig:velocity-test-indoor1}{{3.5b}{24}{Velocity of the refueling port over time (Raw Data).\relax }{figure.caption.37}{}}
\newlabel{sub@fig:velocity-test-indoor1}{{b}{24}{Velocity of the refueling port over time (Raw Data).\relax }{figure.caption.37}{}}
\newlabel{fig:acceleration-test-indoor1}{{3.5c}{24}{Acceleration of the refueling port over time (Raw Data).\relax }{figure.caption.37}{}}
\newlabel{sub@fig:acceleration-test-indoor1}{{c}{24}{Acceleration of the refueling port over time (Raw Data).\relax }{figure.caption.37}{}}
\newlabel{fig:size-test-indoor1}{{3.5d}{24}{Area of the refueling port over time (Raw Data).\relax }{figure.caption.37}{}}
\newlabel{sub@fig:size-test-indoor1}{{d}{24}{Area of the refueling port over time (Raw Data).\relax }{figure.caption.37}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces Temporal analysis of different metrics for the refueling port in the \textit  {test\_indoor1} video (Raw Data). The metrics include (a) central position, (b) velocity, (c) acceleration, and (d) area, providing an overview of the object's dynamics over time.\relax }}{24}{figure.caption.37}\protected@file@percent }
\newlabel{fig:bbox-metrics-test-indoor1}{{3.5}{24}{Temporal analysis of different metrics for the refueling port in the \textit {test\_indoor1} video (Raw Data). The metrics include (a) central position, (b) velocity, (c) acceleration, and (d) area, providing an overview of the object's dynamics over time.\relax }{figure.caption.37}{}}
\newlabel{fig:central-position-test-indoor1-savgol}{{3.6a}{24}{Central position of the refueling port over time (Savgol Filter).\relax }{figure.caption.38}{}}
\newlabel{sub@fig:central-position-test-indoor1-savgol}{{a}{24}{Central position of the refueling port over time (Savgol Filter).\relax }{figure.caption.38}{}}
\newlabel{fig:velocity-test-indoor1-savgol}{{3.6b}{24}{Velocity of the refueling port over time (Savgol Filter).\relax }{figure.caption.38}{}}
\newlabel{sub@fig:velocity-test-indoor1-savgol}{{b}{24}{Velocity of the refueling port over time (Savgol Filter).\relax }{figure.caption.38}{}}
\newlabel{fig:acceleration-test-indoor1-savgol}{{3.6c}{24}{Acceleration of the refueling port over time (Savgol Filter).\relax }{figure.caption.38}{}}
\newlabel{sub@fig:acceleration-test-indoor1-savgol}{{c}{24}{Acceleration of the refueling port over time (Savgol Filter).\relax }{figure.caption.38}{}}
\newlabel{fig:size-test-indoor1-savgol}{{3.6d}{24}{Area of the refueling port over time (Savgol Filter).\relax }{figure.caption.38}{}}
\newlabel{sub@fig:size-test-indoor1-savgol}{{d}{24}{Area of the refueling port over time (Savgol Filter).\relax }{figure.caption.38}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces Temporal analysis of different metrics for the refueling port in the \textit  {test\_indoor1} video (Savgol Filter). The metrics include (a) central position, (b) velocity, (c) acceleration, and (d) area, providing a clearer overview of the object's dynamics over time after applying the Savitzky-Golay filter.\relax }}{24}{figure.caption.38}\protected@file@percent }
\newlabel{fig:bbox-metrics-test-indoor1-savgol}{{3.6}{24}{Temporal analysis of different metrics for the refueling port in the \textit {test\_indoor1} video (Savgol Filter). The metrics include (a) central position, (b) velocity, (c) acceleration, and (d) area, providing a clearer overview of the object's dynamics over time after applying the Savitzky-Golay filter.\relax }{figure.caption.38}{}}
\newlabel{fig:central-position-test-video_lab_platform_6}{{3.7a}{25}{Central position of the refueling port over time (Raw Data).\relax }{figure.caption.39}{}}
\newlabel{sub@fig:central-position-test-video_lab_platform_6}{{a}{25}{Central position of the refueling port over time (Raw Data).\relax }{figure.caption.39}{}}
\newlabel{fig:velocity-test-video_lab_platform_6}{{3.7b}{25}{Velocity of the refueling port over time (Raw Data).\relax }{figure.caption.39}{}}
\newlabel{sub@fig:velocity-test-video_lab_platform_6}{{b}{25}{Velocity of the refueling port over time (Raw Data).\relax }{figure.caption.39}{}}
\newlabel{fig:acceleration-test-video_lab_platform_6}{{3.7c}{25}{Acceleration of the refueling port over time (Raw Data).\relax }{figure.caption.39}{}}
\newlabel{sub@fig:acceleration-test-video_lab_platform_6}{{c}{25}{Acceleration of the refueling port over time (Raw Data).\relax }{figure.caption.39}{}}
\newlabel{fig:size-test-video_lab_platform_6}{{3.7d}{25}{Area of the refueling port over time (Raw Data).\relax }{figure.caption.39}{}}
\newlabel{sub@fig:size-test-video_lab_platform_6}{{d}{25}{Area of the refueling port over time (Raw Data).\relax }{figure.caption.39}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.7}{\ignorespaces Temporal analysis of different metrics for the refueling port in the \textit  {test\_video\_lab\_platform\_6} video (Raw Data). The metrics include (a) central position, (b) velocity, (c) acceleration, and (d) area, providing an overview of the object's dynamics over time.\relax }}{25}{figure.caption.39}\protected@file@percent }
\newlabel{fig:bbox-metrics-test-video_lab_platform_6}{{3.7}{25}{Temporal analysis of different metrics for the refueling port in the \textit {test\_video\_lab\_platform\_6} video (Raw Data). The metrics include (a) central position, (b) velocity, (c) acceleration, and (d) area, providing an overview of the object's dynamics over time.\relax }{figure.caption.39}{}}
\newlabel{fig:central-position-test-video_lab_platform_6-savgol}{{3.8a}{25}{Central position of the refueling port over time (Savgol Filter).\relax }{figure.caption.40}{}}
\newlabel{sub@fig:central-position-test-video_lab_platform_6-savgol}{{a}{25}{Central position of the refueling port over time (Savgol Filter).\relax }{figure.caption.40}{}}
\newlabel{fig:velocity-test-video_lab_platform_6-savgol}{{3.8b}{25}{Velocity of the refueling port over time (Savgol Filter).\relax }{figure.caption.40}{}}
\newlabel{sub@fig:velocity-test-video_lab_platform_6-savgol}{{b}{25}{Velocity of the refueling port over time (Savgol Filter).\relax }{figure.caption.40}{}}
\newlabel{fig:acceleration-test-video_lab_platform_6-savgol}{{3.8c}{25}{Acceleration of the refueling port over time (Savgol Filter).\relax }{figure.caption.40}{}}
\newlabel{sub@fig:acceleration-test-video_lab_platform_6-savgol}{{c}{25}{Acceleration of the refueling port over time (Savgol Filter).\relax }{figure.caption.40}{}}
\newlabel{fig:size-test-video_lab_platform_6-savgol}{{3.8d}{25}{Area of the refueling port over time (Savgol Filter).\relax }{figure.caption.40}{}}
\newlabel{sub@fig:size-test-video_lab_platform_6-savgol}{{d}{25}{Area of the refueling port over time (Savgol Filter).\relax }{figure.caption.40}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.8}{\ignorespaces Temporal analysis of different metrics for the refueling port in the \textit  {test\_video\_lab\_platform\_6} video (Savgol Filter). The metrics include (a) central position, (b) velocity, (c) acceleration, and (d) area, providing a clearer overview of the object's dynamics over time after applying the Savitzky-Golay filter.\relax }}{25}{figure.caption.40}\protected@file@percent }
\newlabel{fig:bbox-metrics-test-video_lab_platform_6-savgol}{{3.8}{25}{Temporal analysis of different metrics for the refueling port in the \textit {test\_video\_lab\_platform\_6} video (Savgol Filter). The metrics include (a) central position, (b) velocity, (c) acceleration, and (d) area, providing a clearer overview of the object's dynamics over time after applying the Savitzky-Golay filter.\relax }{figure.caption.40}{}}
\citation{Jocher_Ultralytics_YOLO_2023}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Framework Design}{26}{section.3.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.9}{\ignorespaces Framework Workflow\relax }}{26}{figure.caption.41}\protected@file@percent }
\newlabel{fig:framework-workflow}{{3.9}{26}{Framework Workflow\relax }{figure.caption.41}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Object Detection Model Fine-tuning}{27}{section.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Sequence Model Design}{27}{section.3.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.10}{\ignorespaces SizPos-GRU Model Architecture\relax }}{27}{figure.caption.42}\protected@file@percent }
\newlabel{fig:sizpos-gru}{{3.10}{27}{SizPos-GRU Model Architecture\relax }{figure.caption.42}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.1}Input Representation}{28}{subsection.3.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.2}Encoders}{28}{subsection.3.4.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.11}{\ignorespaces SizPos-GRU Encoder Architecture. The input sequence \( \mathbf  {X} = \{\mathbf  {x}_1, \mathbf  {x}_2, \dots  , \mathbf  {x}_T\} \) represents either the spatial dynamics vector (\(\mathbf  {P}\)) or the dimensional attributes vector (\(\mathbf  {D}\)). This sequence is processed through multiple GRU layers, producing a sequence of hidden states \( H = \{h_1, h_2, \dots  , h_T\} \) and a final hidden state \( h_T \) that encapsulates the temporal dependencies in the input sequence.\relax }}{29}{figure.caption.43}\protected@file@percent }
\newlabel{fig:sizpos-gru-encoder}{{3.11}{29}{SizPos-GRU Encoder Architecture. The input sequence \( \mathbf {X} = \{\mathbf {x}_1, \mathbf {x}_2, \dots , \mathbf {x}_T\} \) represents either the spatial dynamics vector (\(\mathbf {P}\)) or the dimensional attributes vector (\(\mathbf {D}\)). This sequence is processed through multiple GRU layers, producing a sequence of hidden states \( H = \{h_1, h_2, \dots , h_T\} \) and a final hidden state \( h_T \) that encapsulates the temporal dependencies in the input sequence.\relax }{figure.caption.43}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.3}Hidden State Fusion}{29}{subsection.3.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.4}Decoders}{30}{subsection.3.4.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.12}{\ignorespaces SizPos-GRU Decoder Architecture. This architecture illustrates the decoding process where the input sequence \( \mathbf  {x}_t \) and the last hidden state \( h_{t-1} \) are processed through multiple GRU layers to generate the next hidden state \( h_t \). The sequence of hidden states \( H = \{h_1, h_2, \dots  , h_t\} \) is then passed through a self-attention mechanism, which calculates attention scores and weights. The weighted sum of hidden states is combined with linear and non-linear transformations, including dropout and ReLU activation functions, to produce the final output \( \mathbf  {x}_{t+1} \). This output is used for predicting the next time step in the sequence, continuing the process iteratively for future predictions.\relax }}{32}{figure.caption.44}\protected@file@percent }
\newlabel{fig:sizpos-gru-decoder}{{3.12}{32}{SizPos-GRU Decoder Architecture. This architecture illustrates the decoding process where the input sequence \( \mathbf {x}_t \) and the last hidden state \( h_{t-1} \) are processed through multiple GRU layers to generate the next hidden state \( h_t \). The sequence of hidden states \( H = \{h_1, h_2, \dots , h_t\} \) is then passed through a self-attention mechanism, which calculates attention scores and weights. The weighted sum of hidden states is combined with linear and non-linear transformations, including dropout and ReLU activation functions, to produce the final output \( \mathbf {x}_{t+1} \). This output is used for predicting the next time step in the sequence, continuing the process iteratively for future predictions.\relax }{figure.caption.44}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.5}Algorithm Design}{33}{section.3.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.1}Calculation of Key Values for Loss Functions}{33}{subsection.3.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.2}Loss Function Formulation}{34}{subsection.3.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.3}Data Postprocessing}{35}{subsection.3.5.3}\protected@file@percent }
\citation{Falcon_PyTorch_Lightning_2019}
\citation{Ansel_PyTorch_2_Faster_2024}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Experiment Design}{37}{chapter.4}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:experiment_design}{{4}{37}{Experiment Design}{chapter.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Experiment Environment}{37}{section.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Model Training and Optimization}{37}{section.4.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {4.1}{\ignorespaces Training Configuration Parameters for the SizPos-GRU Model. The values were selected based on optimal results from hyperparameter tuning.\relax }}{38}{table.caption.45}\protected@file@percent }
\newlabel{tab:training_parameters}{{4.1}{38}{Training Configuration Parameters for the SizPos-GRU Model. The values were selected based on optimal results from hyperparameter tuning.\relax }{table.caption.45}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.2}{\ignorespaces Hyperparameter Tuning Configuration\relax }}{38}{table.caption.46}\protected@file@percent }
\newlabel{tab:hp_tuning}{{4.2}{38}{Hyperparameter Tuning Configuration\relax }{table.caption.46}{}}
\citation{DBLP:journals/corr/abs-2010-10270}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Comparison Experiments}{41}{section.4.3}\protected@file@percent }
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces PosVelAcc-LSTM Model\relax }}{42}{algorithm.1}\protected@file@percent }
\newlabel{alg:lstm_posvelacc}{{1}{42}{PosVelAcc-LSTM Model\relax }{algorithm.1}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces SizPos-LSTM Model\relax }}{42}{algorithm.2}\protected@file@percent }
\newlabel{alg:lstm_sizpos}{{2}{42}{SizPos-LSTM Model\relax }{algorithm.2}{}}
\citation{FusionGRU}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Evaluation Metrics}{44}{section.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.5}Framework Evaluation}{46}{section.4.5}\protected@file@percent }
\@writefile{loa}{\contentsline {algorithm}{\numberline {3}{\ignorespaces Object Detection and Future Position Prediction Framework\relax }}{46}{algorithm.3}\protected@file@percent }
\newlabel{alg:framework}{{3}{46}{Object Detection and Future Position Prediction Framework\relax }{algorithm.3}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Results and Discussion}{47}{chapter.5}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:results}{{5}{47}{Results and Discussion}{chapter.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Object Detection Training Results}{47}{section.5.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {5.1}{\ignorespaces Performance comparison of YOLO models (YOLOv10n.pt, YOLOv10s.pt, YOLOv10m.pt) on the classification of fuel port states (CLOSED, SEMI-OPEN, OPEN). The table reports Precision (P), Recall (R), mean Average Precision at IoU=0.5 (mAP50), and mean Average Precision across IoU thresholds from 0.5 to 0.95 (mAP50-95) for each state.\relax }}{47}{table.caption.47}\protected@file@percent }
\newlabel{tab:comparison}{{5.1}{47}{Performance comparison of YOLO models (YOLOv10n.pt, YOLOv10s.pt, YOLOv10m.pt) on the classification of fuel port states (CLOSED, SEMI-OPEN, OPEN). The table reports Precision (P), Recall (R), mean Average Precision at IoU=0.5 (mAP50), and mean Average Precision across IoU thresholds from 0.5 to 0.95 (mAP50-95) for each state.\relax }{table.caption.47}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Experiment Results}{48}{section.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.1}Hyperparameter Tuning}{48}{subsection.5.2.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {5.2}{\ignorespaces Hyperparameter tuning results for trajectory prediction using SizPos-GRU model that leverages 30 past frames (1 sec) to predict the position 60 frames (2 sec) into the future. The table presents the Average Displacement Error (ADE), Final Displacement Error (FDE), Average Intersection over Union (AIoU), and Final Intersection over Union (FIoU) across different configurations of hidden sizes and hidden depths. These metrics help to evaluate the accuracy and spatial consistency of the model’s predictions\relax }}{48}{table.caption.48}\protected@file@percent }
\newlabel{tab:vel-metrics-rounded}{{5.2}{48}{Hyperparameter tuning results for trajectory prediction using SizPos-GRU model that leverages 30 past frames (1 sec) to predict the position 60 frames (2 sec) into the future. The table presents the Average Displacement Error (ADE), Final Displacement Error (FDE), Average Intersection over Union (AIoU), and Final Intersection over Union (FIoU) across different configurations of hidden sizes and hidden depths. These metrics help to evaluate the accuracy and spatial consistency of the model’s predictions\relax }{table.caption.48}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.3}{\ignorespaces Hyperparameter tuning results for trajectory prediction using SizPos-GRU model that leverages 15 past frames (0.5 sec) to predict the position 30 frames (1 sec) into the future. The table presents the Average Displacement Error (ADE), Final Displacement Error (FDE), Average Intersection over Union (AIoU), and Final Intersection over Union (FIoU) across different configurations of hidden sizes and hidden depths. These metrics help to evaluate the accuracy and spatial consistency of the model’s predictions\relax }}{49}{table.caption.49}\protected@file@percent }
\newlabel{tab:new-dataset-adjusted-header}{{5.3}{49}{Hyperparameter tuning results for trajectory prediction using SizPos-GRU model that leverages 15 past frames (0.5 sec) to predict the position 30 frames (1 sec) into the future. The table presents the Average Displacement Error (ADE), Final Displacement Error (FDE), Average Intersection over Union (AIoU), and Final Intersection over Union (FIoU) across different configurations of hidden sizes and hidden depths. These metrics help to evaluate the accuracy and spatial consistency of the model’s predictions\relax }{table.caption.49}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.2}SizPos-GRU Model Evaluation}{49}{subsection.5.2.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {5.4}{\ignorespaces Performance comparison of various models on trajectory prediction tasks using 30 past frames to predict 60 future frames. The table reports the Average Displacement Error (ADE) and Final Displacement Error (FDE) in both pixels and percentage, as well as Average Intersection over Union (AIoU) and Final Intersection over Union (FIoU). Lower ADE and FDE values indicate better accuracy, while higher AIoU and FIoU values indicate better overlap with ground truth. The SizPos-GRU model demonstrates superior performance across all metrics.\relax }}{50}{table.caption.50}\protected@file@percent }
\newlabel{tab:fusion-gru-results-60frames}{{5.4}{50}{Performance comparison of various models on trajectory prediction tasks using 30 past frames to predict 60 future frames. The table reports the Average Displacement Error (ADE) and Final Displacement Error (FDE) in both pixels and percentage, as well as Average Intersection over Union (AIoU) and Final Intersection over Union (FIoU). Lower ADE and FDE values indicate better accuracy, while higher AIoU and FIoU values indicate better overlap with ground truth. The SizPos-GRU model demonstrates superior performance across all metrics.\relax }{table.caption.50}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.5}{\ignorespaces Performance comparison of various models on trajectory prediction tasks using 15 past frames to predict 30 future frames. The table reports the Average Displacement Error (ADE) and Final Displacement Error (FDE) in both pixels and percentage, as well as Average Intersection over Union (AIoU) and Final Intersection over Union (FIoU). Lower ADE and FDE values indicate better accuracy, while higher AIoU and FIoU values indicate better overlap with ground truth. The SizPos-GRU model consistently outperforms the other models across all evaluation metrics.\relax }}{50}{table.caption.51}\protected@file@percent }
\newlabel{tab:fusion-gru-results-30frames}{{5.5}{50}{Performance comparison of various models on trajectory prediction tasks using 15 past frames to predict 30 future frames. The table reports the Average Displacement Error (ADE) and Final Displacement Error (FDE) in both pixels and percentage, as well as Average Intersection over Union (AIoU) and Final Intersection over Union (FIoU). Lower ADE and FDE values indicate better accuracy, while higher AIoU and FIoU values indicate better overlap with ground truth. The SizPos-GRU model consistently outperforms the other models across all evaluation metrics.\relax }{table.caption.51}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.3}Framework Evaluation}{50}{subsection.5.2.3}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {5.6}{\ignorespaces Performance metrics using different smoothing filters and Linear Kalman Filter (LKF) configurations for the video \textit  {video\_lab\_platform\_6}. The table presents Average Displacement Error (ADE), Final Displacement Error (FDE), Average Intersection over Union (AIoU), and Final Intersection over Union (FIoU). The results demonstrate the impact of various filtering techniques on the model’s predictive accuracy.\relax }}{51}{table.caption.52}\protected@file@percent }
\newlabel{tab:performance_metrics}{{5.6}{51}{Performance metrics using different smoothing filters and Linear Kalman Filter (LKF) configurations for the video \textit {video\_lab\_platform\_6}. The table presents Average Displacement Error (ADE), Final Displacement Error (FDE), Average Intersection over Union (AIoU), and Final Intersection over Union (FIoU). The results demonstrate the impact of various filtering techniques on the model’s predictive accuracy.\relax }{table.caption.52}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.7}{\ignorespaces Framework Test with LKF and Savitzky-Golay filter for the videos \textit  {video\_lab\_platform\_6}, \textit  {test\_indoor1}, and \textit  {video\_lab\_semiopen\_1\_\_\_\_\_\_3}. The table presents Average Displacement Error (ADE), Final Displacement Error (FDE), Average Intersection over Union (AIoU), and Final Intersection over Union (FIoU) for each video. The results demonstrate the model’s predictive accuracy across different scenarios.\relax }}{51}{table.caption.53}\protected@file@percent }
\newlabel{tab:framework-test}{{5.7}{51}{Framework Test with LKF and Savitzky-Golay filter for the videos \textit {video\_lab\_platform\_6}, \textit {test\_indoor1}, and \textit {video\_lab\_semiopen\_1\_\_\_\_\_\_3}. The table presents Average Displacement Error (ADE), Final Displacement Error (FDE), Average Intersection over Union (AIoU), and Final Intersection over Union (FIoU) for each video. The results demonstrate the model’s predictive accuracy across different scenarios.\relax }{table.caption.53}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Testing Visualisation}{52}{section.5.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces Visualisation of the trajectory prediction framework output for the \textit  {video\_lab\_platform\_6} and \textit  {test\_indoor1} videos. The figure shows the ground truth trajectory (blue) and the predicted trajectory (red) for the refueling port. The close alignment between the predicted and actual trajectories demonstrates the model’s accuracy in predicting the object’s future positions.\relax }}{52}{figure.caption.54}\protected@file@percent }
\newlabel{fig:trajectory-prediction-test-indoor1}{{5.1}{52}{Visualisation of the trajectory prediction framework output for the \textit {video\_lab\_platform\_6} and \textit {test\_indoor1} videos. The figure shows the ground truth trajectory (blue) and the predicted trajectory (red) for the refueling port. The close alignment between the predicted and actual trajectories demonstrates the model’s accuracy in predicting the object’s future positions.\relax }{figure.caption.54}{}}
\newlabel{fig:framework-video_lab_platform_6-0}{{5.2a}{53}{Predictions from frame 16 to 45.\relax }{figure.caption.55}{}}
\newlabel{sub@fig:framework-video_lab_platform_6-0}{{a}{53}{Predictions from frame 16 to 45.\relax }{figure.caption.55}{}}
\newlabel{fig:framework-video_lab_platform_6-1}{{5.2b}{53}{Predictions from frame 61 to 90.\relax }{figure.caption.55}{}}
\newlabel{sub@fig:framework-video_lab_platform_6-1}{{b}{53}{Predictions from frame 61 to 90.\relax }{figure.caption.55}{}}
\newlabel{fig:framework-video_lab_platform_6-2}{{5.2c}{53}{Predictions from frame 106 to 135.\relax }{figure.caption.55}{}}
\newlabel{sub@fig:framework-video_lab_platform_6-2}{{c}{53}{Predictions from frame 106 to 135.\relax }{figure.caption.55}{}}
\newlabel{fig:framework-video_lab_platform_6-3}{{5.2d}{53}{Predictions from frame 151 to 180.\relax }{figure.caption.55}{}}
\newlabel{sub@fig:framework-video_lab_platform_6-3}{{d}{53}{Predictions from frame 151 to 180.\relax }{figure.caption.55}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces Predicted vs Ground Truth Central Position for video \textit  {video\_lab\_platform\_6} (No Smoothing). \relax }}{53}{figure.caption.55}\protected@file@percent }
\newlabel{fig:framework-video_lab_platform_6}{{5.2}{53}{Predicted vs Ground Truth Central Position for video \textit {video\_lab\_platform\_6} (No Smoothing). \relax }{figure.caption.55}{}}
\newlabel{fig:framework-video_lab_platform_6-savgol-0}{{5.3a}{54}{Predictions from frame 16 to 45.\relax }{figure.caption.56}{}}
\newlabel{sub@fig:framework-video_lab_platform_6-savgol-0}{{a}{54}{Predictions from frame 16 to 45.\relax }{figure.caption.56}{}}
\newlabel{fig:framework-video_lab_platform_6-savgol-1}{{5.3b}{54}{Predictions from frame 61 to 90.\relax }{figure.caption.56}{}}
\newlabel{sub@fig:framework-video_lab_platform_6-savgol-1}{{b}{54}{Predictions from frame 61 to 90.\relax }{figure.caption.56}{}}
\newlabel{fig:framework-video_lab_platform_6-savgol-2}{{5.3c}{54}{Predictions from frame 106 to 135.\relax }{figure.caption.56}{}}
\newlabel{sub@fig:framework-video_lab_platform_6-savgol-2}{{c}{54}{Predictions from frame 106 to 135.\relax }{figure.caption.56}{}}
\newlabel{fig:framework-video_lab_platform_6-savgol-3}{{5.3d}{54}{Predictions from frame 151 to 180.\relax }{figure.caption.56}{}}
\newlabel{sub@fig:framework-video_lab_platform_6-savgol-3}{{d}{54}{Predictions from frame 151 to 180.\relax }{figure.caption.56}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces Predicted vs Ground Truth Central Position for video \textit  {video\_lab\_platform\_6} (Savitzky-Golay \& LKF filter). \relax }}{54}{figure.caption.56}\protected@file@percent }
\newlabel{fig:framework-video_lab_platform_6-savgol}{{5.3}{54}{Predicted vs Ground Truth Central Position for video \textit {video\_lab\_platform\_6} (Savitzky-Golay \& LKF filter). \relax }{figure.caption.56}{}}
\newlabel{fig:framework-test_indoor1-0}{{5.4a}{54}{Predictions from frame 16 to 45.\relax }{figure.caption.57}{}}
\newlabel{sub@fig:framework-test_indoor1-0}{{a}{54}{Predictions from frame 16 to 45.\relax }{figure.caption.57}{}}
\newlabel{fig:framework-test_indoor1-1}{{5.4b}{54}{Predictions from frame 61 to 90.\relax }{figure.caption.57}{}}
\newlabel{sub@fig:framework-test_indoor1-1}{{b}{54}{Predictions from frame 61 to 90.\relax }{figure.caption.57}{}}
\newlabel{fig:framework-test_indoor1-3}{{5.4c}{54}{Predictions from frame 151 to 180.\relax }{figure.caption.57}{}}
\newlabel{sub@fig:framework-test_indoor1-3}{{c}{54}{Predictions from frame 151 to 180.\relax }{figure.caption.57}{}}
\newlabel{fig:framework-test_indoor1-4}{{5.4d}{54}{Predictions from frame 196 to 225.\relax }{figure.caption.57}{}}
\newlabel{sub@fig:framework-test_indoor1-4}{{d}{54}{Predictions from frame 196 to 225.\relax }{figure.caption.57}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.4}{\ignorespaces Predicted vs Ground Truth Central Position for video \textit  {test\_indoor1} (No Smoothing). \relax }}{54}{figure.caption.57}\protected@file@percent }
\newlabel{fig:framework-test_indoor1}{{5.4}{54}{Predicted vs Ground Truth Central Position for video \textit {test\_indoor1} (No Smoothing). \relax }{figure.caption.57}{}}
\newlabel{fig:framework-test_indoor1-savgol-0}{{5.5a}{55}{Predictions from frame 16 to 45.\relax }{figure.caption.58}{}}
\newlabel{sub@fig:framework-test_indoor1-savgol-0}{{a}{55}{Predictions from frame 16 to 45.\relax }{figure.caption.58}{}}
\newlabel{fig:framework-test_indoor1-savgol-1}{{5.5b}{55}{Predictions from frame 61 to 90.\relax }{figure.caption.58}{}}
\newlabel{sub@fig:framework-test_indoor1-savgol-1}{{b}{55}{Predictions from frame 61 to 90.\relax }{figure.caption.58}{}}
\newlabel{fig:framework-test_indoor1-savgol-3}{{5.5c}{55}{Predictions from frame 151 to 180.\relax }{figure.caption.58}{}}
\newlabel{sub@fig:framework-test_indoor1-savgol-3}{{c}{55}{Predictions from frame 151 to 180.\relax }{figure.caption.58}{}}
\newlabel{fig:framework-test_indoor1-savgol-4}{{5.5d}{55}{Predictions from frame 196 to 225.\relax }{figure.caption.58}{}}
\newlabel{sub@fig:framework-test_indoor1-savgol-4}{{d}{55}{Predictions from frame 196 to 225.\relax }{figure.caption.58}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.5}{\ignorespaces Predicted vs Ground Truth Central Position for video \textit  {test\_indoor1} (Savitzky-Golay \& LKF filter). \relax }}{55}{figure.caption.58}\protected@file@percent }
\newlabel{fig:framework-test_indoor1-savgol}{{5.5}{55}{Predicted vs Ground Truth Central Position for video \textit {test\_indoor1} (Savitzky-Golay \& LKF filter). \relax }{figure.caption.58}{}}
\bibstyle{abbrvnat}
\bibdata{LaTeX,CUCitations}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Conclusion and Future Work}{56}{chapter.6}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:conclusion}{{6}{56}{Conclusion and Future Work}{chapter.6}{}}
\bibcite{Alahi2016}{{1}{2016}{{Alahi et~al.}}{{Alahi, Goel, Ramanathan, Robicquet, Fei-Fei, and Savarese}}}
\bibcite{Ansel_PyTorch_2_Faster_2024}{{2}{2024}{{Ansel et~al.}}{{Ansel, Yang, He, Gimelshein, Jain, Voznesensky, Bao, Bell, Berard, Burovski, Chauhan, Chourdia, Constable, Desmaison, DeVito, Ellison, Feng, Gong, Gschwind, Hirsh, Huang, Kalambarkar, Kirsch, Lazos, Lezcano, Liang, Liang, Lu, Luk, Maher, Pan, Puhrsch, Reso, Saroufim, Siraichi, Suk, Suo, Tillet, Wang, Wang, Wen, Zhang, Zhao, Zhou, Zou, Mathews, Chanan, Wu, and Chintala}}}
\bibcite{AR3P2020}{{3}{AvMC}{{}}{{(2020)}}}
\bibcite{ImageRefueling}{{4}{2019}{{Bailey}}{{}}}
\bibcite{Bennett1991}{{5}{1991}{{Bennett et~al.}}{{Bennett, Shiu, and Leahy}}}
\bibcite{blakey2011aviation}{{6}{2011}{{Blakey et~al.}}{{Blakey, Rye, and Wilson}}}
\bibcite{DBLP:journals/corr/abs-2004-10934}{{7}{2020}{{Bochkovskiy et~al.}}{{Bochkovskiy, Wang, and Liao}}}
\bibcite{DBLP:journals/corr/abs-2010-10270}{{8}{2020}{{Bouhsain et~al.}}{{Bouhsain, Saadatnejad, and Alahi}}}
\bibcite{Burnette2010}{{9}{2010}{{Burnette}}{{}}}
\bibcite{SurveyVisualOT}{{10}{2022}{{Chen et~al.}}{{Chen, Wang, Zhao, Lv, and Niu}}}
\@writefile{toc}{\contentsline {chapter}{References}{57}{appendix*.59}\protected@file@percent }
\bibcite{chen2023yoloms}{{11}{2023}{{Chen et~al.}}{{Chen, Yuan, Wu, Wang, Hou, and Cheng}}}
\bibcite{SurveySmallObjectDetection}{{12}{2023}{{Cheng et~al.}}{{Cheng, Yuan, Yao, Yan, Zeng, Xie, and Han}}}
\bibcite{CostsOfUnsafetyAviation}{{13}{2010}{{Cokorilo et~al.}}{{Cokorilo, Gvozdenovic, Vasov, and Mirosavljevic}}}
\bibcite{DBLP:journals/corr/DaiLHS16}{{14}{2016}{{Dai et~al.}}{{Dai, Li, He, and Sun}}}
\bibcite{huggingface2023objectdetection}{{15}{2023}{{Face}}{{}}}
\bibcite{Falcon_PyTorch_Lightning_2019}{{16}{2019}{{Falcon and {The PyTorch Lightning team}}}{{}}}
\bibcite{Ficken2017}{{17}{2017}{{Ficken}}{{}}}
\bibcite{DBLP:journals/corr/abs-2107-08430}{{18}{2021}{{Ge et~al.}}{{Ge, Liu, Wang, Li, and Sun}}}
\bibcite{LearnOpenCVYOLOv10}{{19}{2024}{{Ghosh}}{{}}}
\bibcite{DBLP:journals/corr/Girshick15}{{20}{2015}{{Girshick}}{{}}}
\bibcite{DBLP:journals/corr/GirshickDDM13}{{21}{2013}{{Girshick et~al.}}{{Girshick, Donahue, Darrell, and Malik}}}
\bibcite{AARBinocularVision}{{22}{2023}{{Gong et~al.}}{{Gong, Liu, Xu, Xu, He, Zhang, and Rasol}}}
\bibcite{IntelRealSense}{{23}{2024}{{Intel}}{{}}}
\bibcite{YOLOv5Release}{{24}{2022}{{Jocher}}{{}}}
\bibcite{YOLOv8}{{25}{2023}{{Jocher}}{{}}}
\bibcite{Jocher_Ultralytics_YOLO_2023}{{26}{2023}{{Jocher et~al.}}{{Jocher, Chaurasia, and Qiu}}}
\bibcite{SurveyDLOD}{{27}{2022}{{Kang et~al.}}{{Kang, Tariq, Oh, and Woo}}}
\bibcite{FusionGRU}{{28}{2024}{{Karim et~al.}}{{Karim, Qin, and Wang}}}
\bibcite{DatasetAGR}{{29}{2023}{{Kuang et~al.}}{{Kuang, Barnes, Tang, and Jenkins}}}
\bibcite{SurveyTransformersSingleOT}{{30}{2023}{{Kugarajeevan et~al.}}{{Kugarajeevan, Kokul, Ramanan, and Fernando}}}
\bibcite{FFPSpaceSystemVehicles}{{31}{2022}{{Lee et~al.}}{{Lee, Jeon, Han, and Jeong}}}
\bibcite{li2023yolov6}{{32}{2023}{{Li et~al.}}{{Li, Li, Geng, Jiang, Cheng, Zhang, Ke, Xu, and Chu}}}
\bibcite{lin2018focal}{{33}{2018}{{Lin et~al.}}{{Lin, Goyal, Girshick, He, and Dollár}}}
\bibcite{10.1007/978-981-16-5943-0_26}{{34}{2021{}}{{Liu et~al.}}{{Liu, Chen, and Liu}}}
\bibcite{OverviewCorrelationAlgoOT}{{35}{2021{}}{{Liu et~al.}}{{Liu, Liu, Srivastava, Połap, and Woźniak}}}
\bibcite{DBLP:journals/corr/LiuAESR15}{{36}{2015}{{Liu et~al.}}{{Liu, Anguelov, Erhan, Szegedy, Reed, Fu, and Berg}}}
\bibcite{doi:10.1080/13669877.2013.879493}{{37}{2014}{{Olja~Čokorilo and Dell’Acqua}}{{}}}
\bibcite{ExpertSystemsAGR}{{38}{2021}{{Plaza and Santos}}{{}}}
\bibcite{DBLP:journals/corr/RedmonF16}{{39}{2016}{{Redmon and Farhadi}}{{}}}
\bibcite{DBLP:journals/corr/RedmonDGF15}{{40}{2015}{{Redmon et~al.}}{{Redmon, Divvala, Girshick, and Farhadi}}}
\bibcite{Ren2017}{{41}{2017}{{Ren et~al.}}{{Ren, He, Girshick, and Sun}}}
\bibcite{sati2019aircraft}{{42}{2019}{{Sati et~al.}}{{Sati, Singh, and Yadav}}}
\bibcite{Schultz1986}{{43}{1986}{{Schultz}}{{}}}
\bibcite{MultipleObjectForecasting}{{44}{2020}{{Styles et~al.}}{{Styles, Guha, and Sanchez}}}
\bibcite{wang2024yolov10}{{45}{2024{}}{{Wang et~al.}}{{Wang, Chen, Liu, Chen, Lin, Han, and Ding}}}
\bibcite{wang2023goldyolo}{{46}{2023}{{Wang et~al.}}{{Wang, He, Nie, Guo, Liu, Han, and Wang}}}
\bibcite{wang2024yolov9}{{47}{2024{}}{{Wang et~al.}}{{Wang, Yeh, and Liao}}}
\bibcite{AARCNN}{{48}{2017}{{Wang et~al.}}{{Wang, Dong, Kong, Li, and Zhang}}}
\bibcite{SmallObjectDetectionPositonPrediction}{{49}{2021}{{Wu and Xu}}{{}}}
\bibcite{xu2022ppyoloe}{{50}{2022}{{Xu et~al.}}{{Xu, Wang, Lv, Chang, Cui, Deng, Wang, Dang, Wei, Du, and Lai}}}
\bibcite{xu2023damoyolo}{{51}{2023}{{Xu et~al.}}{{Xu, Jiang, Chen, Huang, Zhang, and Sun}}}
\bibcite{ODNetworkUAVCNNTransformer}{{52}{2023}{{Ye et~al.}}{{Ye, Qin, Zhao, Gao, Deng, and Ouyang}}}
\bibcite{AGRPoseEstimation}{{53}{2021}{{Yildirim et~al.}}{{Yildirim, Rana, and Tang}}}
\bibcite{HybridDatasetAGRV1}{{54}{2023}{{Yildirim et~al.}}{{Yildirim, Rana, and Tang}}}
\bibcite{SurveyModernODModels}{{55}{2022}{{Zaidi et~al.}}{{Zaidi, Ansari, Aslam, Kanwal, Asghar, and Lee}}}
\bibcite{SuveyAdvancesSingleOTMethods}{{56}{2021}{{Zhang et~al.}}{{Zhang, Wang, Liu, Zhang, and Chen}}}
\bibcite{AAREKF}{{57}{2017}{{Zhong et~al.}}{{Zhong, Li, Wang, and Su}}}
\gdef \@abspage@last{73}
