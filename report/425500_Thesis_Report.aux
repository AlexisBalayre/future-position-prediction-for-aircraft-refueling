\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\providecommand \oddpage@label [2]{}
\@writefile{toc}{\contentsline {chapter}{Academic Integrity Declaration}{i}{Doc-Start}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{Table of Contents}{ii}{chapter*.1}\protected@file@percent }
\citation{AAREKF}
\citation{AGRPoseEstimation}
\citation{DatasetAGR}
\citation{huggingface2023objectdetection}
\citation{SurveyDLOD}
\citation{huggingface2023objectdetection}
\citation{CubicLSTMsVideoPrediction}
\citation{UnsupervisedVideoForecastingFlowParsingMechanism}
\citation{VideoFramePredictionByJointOptimizationWithSynthesisAndOpticalFlowEstimation}
\citation{FusionGRU}
\citation{DualBranchSpatialTemporalLearningNetworkVideoPrediction}
\citation{SurveyDLOD}
\citation{DBLP:journals/corr/SrivastavaMS15}
\citation{DBLP:journals/corr/EitelHB17}
\citation{KTH}
\citation{UCFSport}
\@writefile{toc}{\contentsline {chapter}{List of Figures}{iv}{chapter*.2}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{List of Tables}{vi}{chapter*.3}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{List of Abbreviations}{vii}{chapter*.4}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\citation{blakey2011aviation}
\citation{sati2019aircraft}
\citation{blakey2011aviation}
\citation{doi:10.1080/13669877.2013.879493,CostsOfUnsafetyAviation}
\citation{10.1007/978-981-16-5943-0_26}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{1}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Background and Motivation}{1}{section.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces Pressure Refuelling of a Commercial Aircraft. Source: Tom Boon/Simple Flying\relax }}{1}{figure.caption.5}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:pressure-refuelling}{{1.1}{1}{Pressure Refuelling of a Commercial Aircraft. Source: Tom Boon/Simple Flying\relax }{figure.caption.5}{}}
\newlabel{fig:motion-blur-example}{{1.2a}{2}{Motion Blur Example\relax }{figure.caption.6}{}}
\newlabel{sub@fig:motion-blur-example}{{a}{2}{Motion Blur Example\relax }{figure.caption.6}{}}
\newlabel{fig:occlusion-example}{{1.2b}{2}{Occlusion Example\relax }{figure.caption.6}{}}
\newlabel{sub@fig:occlusion-example}{{b}{2}{Occlusion Example\relax }{figure.caption.6}{}}
\newlabel{fig:out-of-view-example}{{1.2c}{2}{Out-of-View Example\relax }{figure.caption.6}{}}
\newlabel{sub@fig:out-of-view-example}{{c}{2}{Out-of-View Example\relax }{figure.caption.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.2}{\ignorespaces Challenges in Detecting Aircraft Refuelling Port\relax }}{2}{figure.caption.6}\protected@file@percent }
\newlabel{fig:challenges-detecting-ports}{{1.2}{2}{Challenges in Detecting Aircraft Refuelling Port\relax }{figure.caption.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Research Gap}{2}{section.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Aim and Objectives}{3}{section.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.4}Technological Contributions}{3}{section.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.5}Thesis Layout}{3}{section.1.5}\protected@file@percent }
\citation{ExpertSystemsAGR}
\citation{Schultz1986}
\citation{Bennett1991}
\citation{Burnette2010}
\citation{Ficken2017}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Literature Review}{4}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:literature-review}{{2}{4}{Literature Review}{chapter.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Automated Refulling Systems in the Aviation Industry}{4}{section.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces AFRL's Automated Aircraft Ground Refueling system prototype robot (Photo Credit: AFRL/RXQ Robotics Group)\relax }}{4}{figure.caption.7}\protected@file@percent }
\newlabel{fig:test-2010}{{2.1}{4}{AFRL's Automated Aircraft Ground Refueling system prototype robot (Photo Credit: AFRL/RXQ Robotics Group)\relax }{figure.caption.7}{}}
\newlabel{fig:test-2010}{{2.1}{4}{AFRL's Automated Aircraft Ground Refueling system prototype robot (Photo Credit: AFRL/RXQ Robotics Group)\relax }{figure.caption.7}{}}
\citation{AR3P2020}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces AR3P Concept Development Prototype Robot (Photo Credit: U.S. Army)\relax }}{5}{figure.caption.8}\protected@file@percent }
\newlabel{fig:test-2017}{{2.2}{5}{AR3P Concept Development Prototype Robot (Photo Credit: U.S. Army)\relax }{figure.caption.8}{}}
\newlabel{test-2020-2}{{2.3a}{5}{AR3P Robot Approaching Detected Aircraft (Photo Credit: Stratom)\relax }{figure.caption.9}{}}
\newlabel{sub@test-2020-2}{{a}{5}{AR3P Robot Approaching Detected Aircraft (Photo Credit: Stratom)\relax }{figure.caption.9}{}}
\newlabel{test-2020-3}{{2.3b}{5}{AR3P Robot Engaging Aircraft Refueling Port (Photo Credit: Stratom)\relax }{figure.caption.9}{}}
\newlabel{sub@test-2020-3}{{b}{5}{AR3P Robot Engaging Aircraft Refueling Port (Photo Credit: Stratom)\relax }{figure.caption.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces AR3P Robot Hot Refueling Demonstration for S-70 Helicopter\relax }}{5}{figure.caption.9}\protected@file@percent }
\newlabel{fig:automated-refuelling-systems}{{2.3}{5}{AR3P Robot Hot Refueling Demonstration for S-70 Helicopter\relax }{figure.caption.9}{}}
\citation{AARBinocularVision,AARCNN}
\citation{AAREKF}
\citation{AAREKF}
\citation{AAREKF}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Autonomous Aerial Refueling (AAR) of X-47B Unmanned Combat Air System Demonstrator (Photo Credit: U.S. Navy)\relax }}{6}{figure.caption.10}\protected@file@percent }
\newlabel{fig:aerial-refuelling}{{2.4}{6}{Autonomous Aerial Refueling (AAR) of X-47B Unmanned Combat Air System Demonstrator (Photo Credit: U.S. Navy)\relax }{figure.caption.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces Autonomous Air Refueling Detection System with EKF. Source: \citet  {AAREKF}\relax }}{6}{figure.caption.11}\protected@file@percent }
\newlabel{fig:detection-system-aarekf}{{2.5}{6}{Autonomous Air Refueling Detection System with EKF. Source: \citet {AAREKF}\relax }{figure.caption.11}{}}
\citation{AGRPoseEstimation}
\citation{AGRPoseEstimation}
\citation{AGRPoseEstimation}
\citation{DatasetAGR}
\citation{HybridDatasetAGRV1}
\citation{DatasetAGR}
\citation{DatasetAGR}
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces Kalman Filter Workflow for Pose Estimation in Autonomous Ground Refueling. Source: \citet  {AGRPoseEstimation}\relax }}{7}{figure.caption.12}\protected@file@percent }
\newlabel{fig:agr-pose-kalman}{{2.6}{7}{Kalman Filter Workflow for Pose Estimation in Autonomous Ground Refueling. Source: \citet {AGRPoseEstimation}\relax }{figure.caption.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces AAGR Dataset Overview. Source: \citet  {DatasetAGR}\relax }}{7}{figure.caption.13}\protected@file@percent }
\newlabel{fig:agr-dataset}{{2.7}{7}{AAGR Dataset Overview. Source: \citet {DatasetAGR}\relax }{figure.caption.13}{}}
\citation{huggingface2023objectdetection}
\citation{huggingface2023objectdetection}
\citation{huggingface2023objectdetection}
\citation{DBLP:journals/corr/GirshickDDM13}
\citation{DBLP:journals/corr/Girshick15}
\citation{Ren2017}
\citation{DBLP:journals/corr/DaiLHS16}
\citation{SurveyDLOD,ODNetworkUAVCNNTransformer}
\citation{DBLP:journals/corr/LiuAESR15}
\citation{DBLP:journals/corr/RedmonDGF15,DBLP:journals/corr/RedmonF16,DBLP:journals/corr/abs-2004-10934,chen2023yoloms,DBLP:journals/corr/abs-2107-08430,YOLOv5Release,li2023yolov6,YOLOv8,wang2024yolov9,xu2022ppyoloe,wang2023goldyolo,xu2023damoyolo,wang2024yolov10}
\citation{lin2018focal}
\citation{SurveyDLOD,ODNetworkUAVCNNTransformer}
\citation{SurveyDLOD}
\citation{SurveyDLOD}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Object Detection and Tracking in Computer Vision}{8}{section.2.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.8}{\ignorespaces Example of outputs from an object detector\nobreakspace  {}\cite  {huggingface2023objectdetection}.\relax }}{8}{figure.caption.14}\protected@file@percent }
\newlabel{fig:object-detection}{{2.8}{8}{Example of outputs from an object detector~\cite {huggingface2023objectdetection}.\relax }{figure.caption.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.9}{\ignorespaces Basic deep learning-based one-stage vs two-stage object detection model architectures\nobreakspace  {}\cite  {SurveyDLOD}.\relax }}{8}{figure.caption.15}\protected@file@percent }
\newlabel{fig:two-stage-vs-single-stage}{{2.9}{8}{Basic deep learning-based one-stage vs two-stage object detection model architectures~\cite {SurveyDLOD}.\relax }{figure.caption.15}{}}
\citation{huggingface2023objectdetection}
\citation{huggingface2023objectdetection}
\citation{huggingface2023objectdetection}
\citation{huggingface2023objectdetection}
\citation{huggingface2023objectdetection}
\citation{huggingface2023objectdetection}
\citation{huggingface2023objectdetection}
\citation{huggingface2023objectdetection}
\citation{SurveyVisualOT}
\citation{SurveyVisualOT}
\citation{OverviewCorrelationAlgoOT,SuveyAdvancesSingleOTMethods,SurveyModernODModels}
\citation{SuveyAdvancesSingleOTMethods,SurveyModernODModels,SurveyTransformersSingleOT}
\citation{SurveyTransformersSingleOT}
\citation{SurveySmallObjectDetection,SmallObjectDetectionPositonPrediction}
\citation{SuveyAdvancesSingleOTMethods,SurveyModernODModels}
\citation{OverviewCorrelationAlgoOT,SuveyAdvancesSingleOTMethods}
\@writefile{lof}{\contentsline {figure}{\numberline {2.10}{\ignorespaces Intersection over Union (IoU) between a detection (in green) and ground-truth (in blue).\nobreakspace  {}\cite  {huggingface2023objectdetection}\relax }}{9}{figure.caption.16}\protected@file@percent }
\newlabel{fig:iou-metric}{{2.10}{9}{Intersection over Union (IoU) between a detection (in green) and ground-truth (in blue).~\cite {huggingface2023objectdetection}\relax }{figure.caption.16}{}}
\citation{FFPSpaceSystemVehicles}
\citation{Alahi2016}
\citation{Vaswani2017}
\citation{HowDoUGoWhere}
\citation{ConvLSTM}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Deep Learning for Spacio-Temporal Prediction}{11}{section.2.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.11}{\ignorespaces Comparing different Sequence models: RNN, LSTM, and GRU. Source: Colah's blog. Compiled by AIML.com\relax }}{11}{figure.caption.17}\protected@file@percent }
\newlabel{fig:lstm-rnn-gru}{{2.11}{11}{Comparing different Sequence models: RNN, LSTM, and GRU. Source: Colah's blog. Compiled by AIML.com\relax }{figure.caption.17}{}}
\citation{CubicLSTMsVideoPrediction}
\citation{CubicLSTMsVideoPrediction}
\citation{CubicLSTMsVideoPrediction}
\citation{UnsupervisedVideoForecastingFlowParsingMechanism}
\@writefile{lof}{\contentsline {figure}{\numberline {2.12}{\ignorespaces Cubic LSTM Architecture. (a) 3D structure of the CubicLSTM unit. (b) Topological diagram of the CubicLSTM unit. (c) Two-spatial-layer RNN composed of CubicLSTM units. The unit consists of three branches, a spatial (z-) branch for extracting and recognizing moving objects, a temporal (y-) branch for capturing and predicting motions, and an output (x-) branch for combining the first two branches to generate the predicted frames. Source:\nobreakspace  {}\citet  {CubicLSTMsVideoPrediction}\relax }}{12}{figure.caption.18}\protected@file@percent }
\newlabel{fig:cubic-lstm}{{2.12}{12}{Cubic LSTM Architecture. (a) 3D structure of the CubicLSTM unit. (b) Topological diagram of the CubicLSTM unit. (c) Two-spatial-layer RNN composed of CubicLSTM units. The unit consists of three branches, a spatial (z-) branch for extracting and recognizing moving objects, a temporal (y-) branch for capturing and predicting motions, and an output (x-) branch for combining the first two branches to generate the predicted frames. Source:~\citet {CubicLSTMsVideoPrediction}\relax }{figure.caption.18}{}}
\citation{UnsupervisedVideoForecastingFlowParsingMechanism}
\citation{UnsupervisedVideoForecastingFlowParsingMechanism}
\citation{VideoFramePredictionByJointOptimizationWithSynthesisAndOpticalFlowEstimation}
\citation{VideoFramePredictionByJointOptimizationWithSynthesisAndOpticalFlowEstimation}
\citation{VideoFramePredictionByJointOptimizationWithSynthesisAndOpticalFlowEstimation}
\@writefile{lof}{\contentsline {figure}{\numberline {2.13}{\ignorespaces Model Architecture. Source:\nobreakspace  {}\citet  {UnsupervisedVideoForecastingFlowParsingMechanism}\relax }}{13}{figure.caption.19}\protected@file@percent }
\newlabel{fig:unsupervised-video-forecasting-flow-parsing}{{2.13}{13}{Model Architecture. Source:~\citet {UnsupervisedVideoForecastingFlowParsingMechanism}\relax }{figure.caption.19}{}}
\citation{FusionGRU}
\citation{FusionGRU}
\citation{FusionGRU}
\citation{DualBranchSpatialTemporalLearningNetworkVideoPrediction}
\@writefile{lof}{\contentsline {figure}{\numberline {2.14}{\ignorespaces FPNet-OF model architecture. Source:\nobreakspace  {}\citet  {VideoFramePredictionByJointOptimizationWithSynthesisAndOpticalFlowEstimation}\relax }}{14}{figure.caption.20}\protected@file@percent }
\newlabel{fig:joint-optimization-synthesis-optical-flow}{{2.14}{14}{FPNet-OF model architecture. Source:~\citet {VideoFramePredictionByJointOptimizationWithSynthesisAndOpticalFlowEstimation}\relax }{figure.caption.20}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.15}{\ignorespaces Fusion-GRU model architecture. Source:\nobreakspace  {}\citet  {FusionGRU}\relax }}{14}{figure.caption.21}\protected@file@percent }
\newlabel{fig:fusion-gru}{{2.15}{14}{Fusion-GRU model architecture. Source:~\citet {FusionGRU}\relax }{figure.caption.21}{}}
\citation{DualBranchSpatialTemporalLearningNetworkVideoPrediction}
\citation{DualBranchSpatialTemporalLearningNetworkVideoPrediction}
\citation{SurveyDLOD}
\citation{SurveyDLOD}
\citation{DBLP:journals/corr/SrivastavaMS15}
\citation{CubicLSTMsVideoPrediction}
\@writefile{lof}{\contentsline {figure}{\numberline {2.16}{\ignorespaces Dual-Branch Spatial-Temporal Learning Network. Source:\nobreakspace  {}\citet  {DualBranchSpatialTemporalLearningNetworkVideoPrediction}\relax }}{15}{figure.caption.22}\protected@file@percent }
\newlabel{fig:dual-branch-spatial-temporal-learning-network}{{2.16}{15}{Dual-Branch Spatial-Temporal Learning Network. Source:~\citet {DualBranchSpatialTemporalLearningNetworkVideoPrediction}\relax }{figure.caption.22}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.17}{\ignorespaces Dual-Branch Spatial-Temporal Learning Network. Source:\nobreakspace  {}\citet  {SurveyDLOD}\relax }}{15}{figure.caption.23}\protected@file@percent }
\newlabel{fig:dual-branch-inter-intra-frames}{{2.17}{15}{Dual-Branch Spatial-Temporal Learning Network. Source:~\citet {SurveyDLOD}\relax }{figure.caption.23}{}}
\citation{DBLP:journals/corr/SrivastavaMS15}
\citation{DBLP:journals/corr/SrivastavaMS15}
\citation{DBLP:journals/corr/FinnGL16}
\citation{CubicLSTMsVideoPrediction}
\citation{DBLP:journals/corr/EitelHB17}
\citation{DBLP:journals/corr/EitelHB17}
\citation{KTH}
\citation{CubicLSTMsVideoPrediction}
\citation{KTH}
\citation{KTH}
\@writefile{lof}{\contentsline {figure}{\numberline {2.18}{\ignorespaces 2-digit Moving MNIST data by\nobreakspace  {}\citet  {DBLP:journals/corr/SrivastavaMS15}\relax }}{16}{figure.caption.24}\protected@file@percent }
\newlabel{fig:moving-mnist}{{2.18}{16}{2-digit Moving MNIST data by~\citet {DBLP:journals/corr/SrivastavaMS15}\relax }{figure.caption.24}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.19}{\ignorespaces Robotic Pushing Dataset. Source:\nobreakspace  {}\citet  {DBLP:journals/corr/EitelHB17}\relax }}{16}{figure.caption.25}\protected@file@percent }
\newlabel{fig:robotic-pushing-dataset}{{2.19}{16}{Robotic Pushing Dataset. Source:~\citet {DBLP:journals/corr/EitelHB17}\relax }{figure.caption.25}{}}
\citation{UCFSport}
\citation{DualBranchSpatialTemporalLearningNetworkVideoPrediction}
\citation{UCFSport}
\citation{UCFSport}
\@writefile{lof}{\contentsline {figure}{\numberline {2.20}{\ignorespaces KTH Action Dataset. Source:\nobreakspace  {}\citet  {KTH}\relax }}{17}{figure.caption.26}\protected@file@percent }
\newlabel{fig:kth-action-dataset}{{2.20}{17}{KTH Action Dataset. Source:~\citet {KTH}\relax }{figure.caption.26}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.21}{\ignorespaces UCF Sports Dataset. Source:\nobreakspace  {}\citet  {UCFSport}\relax }}{17}{figure.caption.27}\protected@file@percent }
\newlabel{fig:ucf-sports-dataset}{{2.21}{17}{UCF Sports Dataset. Source:~\citet {UCFSport}\relax }{figure.caption.27}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Methodology}{18}{chapter.3}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:methodology}{{3}{18}{Methodology}{chapter.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Dataset Configuration}{18}{section.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}Provided Dataset Description}{18}{subsection.3.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.2}Data Annotation}{18}{subsection.3.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.3}Summary of Available Videos}{19}{subsection.3.1.3}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {3.1}{\ignorespaces \centering  Summary of available videos in the HARD dataset with their assignment.\relax }}{20}{table.caption.28}\protected@file@percent }
\newlabel{tab:video_summary}{{3.1}{20}{\centering Summary of available videos in the HARD dataset with their assignment.\relax }{table.caption.28}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.4}Initial Data Distribution}{20}{subsection.3.1.4}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {3.2}{\ignorespaces \centering  Distribution of frames across train, test, and validation sets for each state in the HARD dataset before balancing.\relax }}{20}{table.caption.29}\protected@file@percent }
\newlabel{tab:frame_distribution}{{3.2}{20}{\centering Distribution of frames across train, test, and validation sets for each state in the HARD dataset before balancing.\relax }{table.caption.29}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.5}Balanced Data Distribution}{21}{subsection.3.1.5}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {3.3}{\ignorespaces \centering  Distribution of frames across train, test, and validation sets for each state in the HARD dataset after balancing.\relax }}{21}{table.caption.30}\protected@file@percent }
\newlabel{tab:balanced_frame_distribution}{{3.3}{21}{\centering Distribution of frames across train, test, and validation sets for each state in the HARD dataset after balancing.\relax }{table.caption.30}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.6}Example Images from the Dataset}{21}{subsection.3.1.6}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Annotated images of the fueling port in the CLOSED state.\relax }}{22}{figure.caption.31}\protected@file@percent }
\newlabel{fig:grid-closed-images}{{3.1}{22}{Annotated images of the fueling port in the CLOSED state.\relax }{figure.caption.31}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Annotated images of the fueling port in the OPEN state.\relax }}{22}{figure.caption.32}\protected@file@percent }
\newlabel{fig:grid-open-images}{{3.2}{22}{Annotated images of the fueling port in the OPEN state.\relax }{figure.caption.32}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Annotated images of the fueling port in the SEMI-OPEN state.\relax }}{22}{figure.caption.33}\protected@file@percent }
\newlabel{fig:grid-semi-open-images}{{3.3}{22}{Annotated images of the fueling port in the SEMI-OPEN state.\relax }{figure.caption.33}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Framework Design}{23}{section.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Model Design}{23}{section.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}LSTM-based Sequence Model}{23}{subsection.3.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.1.1}Input Representation}{23}{subsubsection.3.3.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.1.2}Encoders}{23}{subsubsection.3.3.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.1.3}Attention Mechanism}{24}{subsubsection.3.3.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}Decoders}{24}{subsection.3.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.3}Transformer-Based Architecture}{25}{subsection.3.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.3.1}Input Representation}{25}{subsubsection.3.3.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.3.2}Encoder}{25}{subsubsection.3.3.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.3.3}Decoder}{26}{subsubsection.3.3.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.3.4}Multi-Head Self-Attention}{26}{subsubsection.3.3.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.3.5}Output Generation}{26}{subsubsection.3.3.3.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.3.6}Loss Function and Training}{26}{subsubsection.3.3.3.6}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Algorithm Design}{27}{section.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.5}Data Augmentation Strategy}{27}{section.3.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.1}Sequence Reversal}{27}{subsection.3.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.2}Camera Movement Simulation}{27}{subsection.3.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.3}Zoom Simulation}{27}{subsection.3.5.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.4}Detection Inaccuracy Simulation}{28}{subsection.3.5.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.5}Implementation Details}{28}{subsection.3.5.5}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Experiment Design}{29}{chapter.4}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:experiment_design}{{4}{29}{Experiment Design}{chapter.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Experiment Environment}{29}{section.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Comparison Experiments}{29}{section.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Results and Discussion}{30}{chapter.5}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:results}{{5}{30}{Results and Discussion}{chapter.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Object Detection Training Results}{30}{section.5.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {5.1}{\ignorespaces Comparison of YOLO Models\relax }}{30}{table.caption.34}\protected@file@percent }
\newlabel{tab:comparison}{{5.1}{30}{Comparison of YOLO Models\relax }{table.caption.34}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.1}Summary}{30}{subsection.5.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Data Description}{31}{section.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Experiment Results}{31}{section.5.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.4}Testing Visualisation}{31}{section.5.4}\protected@file@percent }
\bibstyle{abbrvnat}
\bibdata{LaTeX,CUCitations}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Conclusion and Future Work}{32}{chapter.6}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:conclusion}{{6}{32}{Conclusion and Future Work}{chapter.6}{}}
\bibcite{Alahi2016}{{1}{2016}{{Alahi et~al.}}{{Alahi, Goel, Ramanathan, Robicquet, Fei-Fei, and Savarese}}}
\bibcite{AR3P2020}{{2}{AvMC}{{}}{{(2020)}}}
\bibcite{Bennett1991}{{3}{1991}{{Bennett et~al.}}{{Bennett, Shiu, and Leahy}}}
\bibcite{blakey2011aviation}{{4}{2011}{{Blakey et~al.}}{{Blakey, Rye, and Wilson}}}
\bibcite{DBLP:journals/corr/abs-2004-10934}{{5}{2020}{{Bochkovskiy et~al.}}{{Bochkovskiy, Wang, and Liao}}}
\bibcite{Burnette2010}{{6}{2010}{{Burnette}}{{}}}
\bibcite{SurveyVisualOT}{{7}{2022}{{Chen et~al.}}{{Chen, Wang, Zhao, Lv, and Niu}}}
\bibcite{chen2023yoloms}{{8}{2023}{{Chen et~al.}}{{Chen, Yuan, Wu, Wang, Hou, and Cheng}}}
\bibcite{SurveySmallObjectDetection}{{9}{2023}{{Cheng et~al.}}{{Cheng, Yuan, Yao, Yan, Zeng, Xie, and Han}}}
\bibcite{CostsOfUnsafetyAviation}{{10}{2010}{{Cokorilo et~al.}}{{Cokorilo, Gvozdenovic, Vasov, and Mirosavljevic}}}
\bibcite{DBLP:journals/corr/DaiLHS16}{{11}{2016}{{Dai et~al.}}{{Dai, Li, He, and Sun}}}
\bibcite{DBLP:journals/corr/EitelHB17}{{12}{2017}{{Eitel et~al.}}{{Eitel, Hauff, and Burgard}}}
\@writefile{toc}{\contentsline {chapter}{References}{33}{chapter*.35}\protected@file@percent }
\bibcite{huggingface2023objectdetection}{{13}{2023}{{Face}}{{}}}
\bibcite{CubicLSTMsVideoPrediction}{{14}{2019}{{Fan et~al.}}{{Fan, Zhu, and Yang}}}
\bibcite{Ficken2017}{{15}{2017}{{Ficken}}{{}}}
\bibcite{DBLP:journals/corr/FinnGL16}{{16}{2016}{{Finn et~al.}}{{Finn, Goodfellow, and Levine}}}
\bibcite{DBLP:journals/corr/abs-2107-08430}{{17}{2021}{{Ge et~al.}}{{Ge, Liu, Wang, Li, and Sun}}}
\bibcite{DBLP:journals/corr/Girshick15}{{18}{2015}{{Girshick}}{{}}}
\bibcite{DBLP:journals/corr/GirshickDDM13}{{19}{2013}{{Girshick et~al.}}{{Girshick, Donahue, Darrell, and Malik}}}
\bibcite{AARBinocularVision}{{20}{2023}{{Gong et~al.}}{{Gong, Liu, Xu, Xu, He, Zhang, and Rasol}}}
\bibcite{HowDoUGoWhere}{{21}{2022}{{Hong et~al.}}{{Hong, Martin, and Raubal}}}
\bibcite{DualBranchSpatialTemporalLearningNetworkVideoPrediction}{{22}{2024}{{Huang and Guan}}{{}}}
\bibcite{UnsupervisedVideoForecastingFlowParsingMechanism}{{23}{2024}{{Jin et~al.}}{{Jin, Song, Li, and Zhang}}}
\bibcite{YOLOv5Release}{{24}{2022}{{Jocher}}{{}}}
\bibcite{YOLOv8}{{25}{2023}{{Jocher}}{{}}}
\bibcite{SurveyDLOD}{{26}{2022}{{Kang et~al.}}{{Kang, Tariq, Oh, and Woo}}}
\bibcite{FusionGRU}{{27}{2024}{{Karim et~al.}}{{Karim, Qin, and Wang}}}
\bibcite{DatasetAGR}{{28}{2023}{{Kuang et~al.}}{{Kuang, Barnes, Tang, and Jenkins}}}
\bibcite{SurveyTransformersSingleOT}{{29}{2023}{{Kugarajeevan et~al.}}{{Kugarajeevan, Kokul, Ramanan, and Fernando}}}
\bibcite{FFPSpaceSystemVehicles}{{30}{2022}{{Lee et~al.}}{{Lee, Jeon, Han, and Jeong}}}
\bibcite{li2023yolov6}{{31}{2023}{{Li et~al.}}{{Li, Li, Geng, Jiang, Cheng, Zhang, Ke, Xu, and Chu}}}
\bibcite{lin2018focal}{{32}{2018}{{Lin et~al.}}{{Lin, Goyal, Girshick, He, and Dollár}}}
\bibcite{10.1007/978-981-16-5943-0_26}{{33}{2021{}}{{Liu et~al.}}{{Liu, Chen, and Liu}}}
\bibcite{OverviewCorrelationAlgoOT}{{34}{2021{}}{{Liu et~al.}}{{Liu, Liu, Srivastava, Połap, and Woźniak}}}
\bibcite{DBLP:journals/corr/LiuAESR15}{{35}{2015}{{Liu et~al.}}{{Liu, Anguelov, Erhan, Szegedy, Reed, Fu, and Berg}}}
\bibcite{doi:10.1080/13669877.2013.879493}{{36}{2014}{{Olja~Čokorilo and Dell’Acqua}}{{}}}
\bibcite{ExpertSystemsAGR}{{37}{2021}{{Plaza and Santos}}{{}}}
\bibcite{VideoFramePredictionByJointOptimizationWithSynthesisAndOpticalFlowEstimation}{{38}{2023}{{Ranjan et~al.}}{{Ranjan, Bhandari, Kim, and Kim}}}
\bibcite{DBLP:journals/corr/RedmonF16}{{39}{2016}{{Redmon and Farhadi}}{{}}}
\bibcite{DBLP:journals/corr/RedmonDGF15}{{40}{2015}{{Redmon et~al.}}{{Redmon, Divvala, Girshick, and Farhadi}}}
\bibcite{Ren2017}{{41}{2017}{{Ren et~al.}}{{Ren, He, Girshick, and Sun}}}
\bibcite{UCFSport}{{42}{2008}{{Rodriguez et~al.}}{{Rodriguez, Ahmed, and Shah}}}
\bibcite{sati2019aircraft}{{43}{2019}{{Sati et~al.}}{{Sati, Singh, and Yadav}}}
\bibcite{KTH}{{44}{2004}{{Schuldt et~al.}}{{Schuldt, Laptev, and Caputo}}}
\bibcite{Schultz1986}{{45}{1986}{{Schultz}}{{}}}
\bibcite{ConvLSTM}{{46}{2015}{{Shi et~al.}}{{Shi, Chen, Wang, Yeung, Wong, and Woo}}}
\bibcite{DBLP:journals/corr/SrivastavaMS15}{{47}{2015}{{Srivastava et~al.}}{{Srivastava, Mansimov, and Salakhutdinov}}}
\bibcite{Vaswani2017}{{48}{2017}{{Vaswani et~al.}}{{Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Łukasz Kaiser, and Polosukhin}}}
\bibcite{wang2024yolov10}{{49}{2024{}}{{Wang et~al.}}{{Wang, Chen, Liu, Chen, Lin, Han, and Ding}}}
\bibcite{wang2023goldyolo}{{50}{2023}{{Wang et~al.}}{{Wang, He, Nie, Guo, Liu, Han, and Wang}}}
\bibcite{wang2024yolov9}{{51}{2024{}}{{Wang et~al.}}{{Wang, Yeh, and Liao}}}
\bibcite{AARCNN}{{52}{2017}{{Wang et~al.}}{{Wang, Dong, Kong, Li, and Zhang}}}
\bibcite{SmallObjectDetectionPositonPrediction}{{53}{2021}{{Wu and Xu}}{{}}}
\bibcite{xu2022ppyoloe}{{54}{2022}{{Xu et~al.}}{{Xu, Wang, Lv, Chang, Cui, Deng, Wang, Dang, Wei, Du, and Lai}}}
\bibcite{xu2023damoyolo}{{55}{2023}{{Xu et~al.}}{{Xu, Jiang, Chen, Huang, Zhang, and Sun}}}
\bibcite{ODNetworkUAVCNNTransformer}{{56}{2023}{{Ye et~al.}}{{Ye, Qin, Zhao, Gao, Deng, and Ouyang}}}
\bibcite{AGRPoseEstimation}{{57}{2021}{{Yildirim et~al.}}{{Yildirim, Rana, and Tang}}}
\bibcite{HybridDatasetAGRV1}{{58}{2023}{{Yildirim et~al.}}{{Yildirim, Rana, and Tang}}}
\bibcite{SurveyModernODModels}{{59}{2022}{{Zaidi et~al.}}{{Zaidi, Ansari, Aslam, Kanwal, Asghar, and Lee}}}
\bibcite{SuveyAdvancesSingleOTMethods}{{60}{2021}{{Zhang et~al.}}{{Zhang, Wang, Liu, Zhang, and Chen}}}
\bibcite{AAREKF}{{61}{2017}{{Zhong et~al.}}{{Zhong, Li, Wang, and Su}}}
\gdef \@abspage@last{46}
