\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\providecommand \oddpage@label [2]{}
\@writefile{toc}{\contentsline {chapter}{Academic Integrity Declaration}{i}{Doc-Start}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{Table of Contents}{ii}{chapter*.1}\protected@file@percent }
\citation{huggingface2023objectdetection}
\citation{SurveyDLOD}
\citation{huggingface2023objectdetection}
\citation{CubicLSTMsVideoPrediction}
\citation{UnsupervisedVideoForecastingFlowParsingMechanism}
\citation{VideoFramePredictionByJointOptimizationWithSynthesisAndOpticalFlowEstimation}
\citation{FusionGRU}
\citation{DualBranchSpatialTemporalLearningNetworkVideoPrediction}
\citation{SurveyDLOD}
\citation{DBLP:journals/corr/SrivastavaMS15}
\citation{DBLP:journals/corr/EitelHB17}
\citation{KTH}
\citation{UCFSport}
\@writefile{toc}{\contentsline {chapter}{List of Figures}{iii}{chapter*.2}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{List of Tables}{iv}{chapter*.3}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{List of Abbreviations}{v}{chapter*.4}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\citation{blakey2011aviation}
\citation{kazda2015airport}
\citation{sati2019aircraft}
\citation{iata2019guidance}
\citation{sati2019aircraft}
\citation{iata2019guidance}
\citation{blakey2011aviation}
\citation{kazda2015airport}
\citation{iata2019guidance}
\citation{sati2019aircraft}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{1}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces Pressure Refuelling of a Commercial Aircraft. Source: Tom Boon/Simple Flying\relax }}{1}{figure.caption.5}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:pressure-refuelling}{{1.1}{1}{Pressure Refuelling of a Commercial Aircraft. Source: Tom Boon/Simple Flying\relax }{figure.caption.5}{}}
\citation{Schultz1986,Bennett1991,DatasetAGR}
\citation{HybridDatasetAGRV2}
\citation{AGRPoseEstimation}
\citation{AARBinocularVision}
\citation{DatasetAGR}
\citation{AGRPoseEstimation}
\citation{HybridDatasetAGRV1}
\citation{HybridDatasetAGRV2}
\citation{AGRPoseEstimation}
\citation{DatasetAGR}
\citation{HybridDatasetAGRV1}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Literature Review}{3}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Automated Refulling Systems in the Aviation Industry}{3}{section.2.1}\protected@file@percent }
\citation{AARCNN}
\citation{Chen2011}
\citation{AARCNN}
\citation{AAREKF}
\citation{huggingface2023objectdetection}
\citation{huggingface2023objectdetection}
\citation{huggingface2023objectdetection}
\citation{DBLP:journals/corr/GirshickDDM13}
\citation{DBLP:journals/corr/Girshick15}
\citation{Ren2017}
\citation{DBLP:journals/corr/DaiLHS16}
\citation{SurveyDLOD,ODNetworkUAVCNNTransformer}
\citation{DBLP:journals/corr/LiuAESR15}
\citation{DBLP:journals/corr/RedmonDGF15,DBLP:journals/corr/RedmonF16,DBLP:journals/corr/abs-2004-10934,chen2023yoloms,DBLP:journals/corr/abs-2107-08430,YOLOv5Release,li2023yolov6,YOLOv8,wang2024yolov9,xu2022ppyoloe,wang2023goldyolo,xu2023damoyolo,wang2024yolov10}
\citation{lin2018focal}
\citation{SurveyDLOD,ODNetworkUAVCNNTransformer}
\citation{SurveyDLOD}
\citation{SurveyDLOD}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Object Detection and Tracking in Computer Vision}{5}{section.2.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Example of outputs from an object detector\nobreakspace  {}\cite  {huggingface2023objectdetection}.\relax }}{5}{figure.caption.6}\protected@file@percent }
\newlabel{fig:object-detection}{{2.1}{5}{Example of outputs from an object detector~\cite {huggingface2023objectdetection}.\relax }{figure.caption.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Basic deep learning-based one-stage vs two-stage object detection model architectures\nobreakspace  {}\cite  {SurveyDLOD}.\relax }}{5}{figure.caption.7}\protected@file@percent }
\newlabel{fig:two-stage-vs-single-stage}{{2.2}{5}{Basic deep learning-based one-stage vs two-stage object detection model architectures~\cite {SurveyDLOD}.\relax }{figure.caption.7}{}}
\citation{huggingface2023objectdetection}
\citation{huggingface2023objectdetection}
\citation{huggingface2023objectdetection}
\citation{huggingface2023objectdetection}
\citation{huggingface2023objectdetection}
\citation{huggingface2023objectdetection}
\citation{huggingface2023objectdetection}
\citation{huggingface2023objectdetection}
\citation{SurveyVisualOT}
\citation{SurveyVisualOT}
\citation{OverviewCorrelationAlgoOT,SuveyAdvancesSingleOTMethods,SurveyModernODModels}
\citation{SuveyAdvancesSingleOTMethods,SurveyModernODModels,SurveyTransformersSingleOT}
\citation{SurveyTransformersSingleOT}
\citation{SurveySmallObjectDetection,SmallObjectDetectionPositonPrediction}
\citation{SuveyAdvancesSingleOTMethods,SurveyModernODModels}
\citation{OverviewCorrelationAlgoOT,SuveyAdvancesSingleOTMethods}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Intersection over Union (IoU) between a detection (in green) and ground-truth (in blue).\nobreakspace  {}\cite  {huggingface2023objectdetection}\relax }}{6}{figure.caption.8}\protected@file@percent }
\newlabel{fig:iou-metric}{{2.3}{6}{Intersection over Union (IoU) between a detection (in green) and ground-truth (in blue).~\cite {huggingface2023objectdetection}\relax }{figure.caption.8}{}}
\citation{FFPSpaceSystemVehicles}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Deep Learning for Spacio-Temporal Prediction}{7}{section.2.3}\protected@file@percent }
\citation{Alahi2016}
\citation{Vaswani2017}
\citation{HowDoUGoWhere}
\citation{ConvLSTM}
\citation{CubicLSTMsVideoPrediction}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Comparing different Sequence models: RNN, LSTM, and GRU. Source: Colah's blog. Compiled by AIML.com\relax }}{8}{figure.caption.9}\protected@file@percent }
\newlabel{fig:lstm-rnn-gru}{{2.4}{8}{Comparing different Sequence models: RNN, LSTM, and GRU. Source: Colah's blog. Compiled by AIML.com\relax }{figure.caption.9}{}}
\citation{CubicLSTMsVideoPrediction}
\citation{CubicLSTMsVideoPrediction}
\citation{UnsupervisedVideoForecastingFlowParsingMechanism}
\citation{UnsupervisedVideoForecastingFlowParsingMechanism}
\citation{UnsupervisedVideoForecastingFlowParsingMechanism}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces Cubic LSTM Architecture. (a) 3D structure of the CubicLSTM unit. (b) Topological diagram of the CubicLSTM unit. (c) Two-spatial-layer RNN composed of CubicLSTM units. The unit consists of three branches, a spatial (z-) branch for extracting and recognizing moving objects, a temporal (y-) branch for capturing and predicting motions, and an output (x-) branch for combining the first two branches to generate the predicted frames. Source:\nobreakspace  {}\citet  {CubicLSTMsVideoPrediction}\relax }}{9}{figure.caption.10}\protected@file@percent }
\newlabel{fig:cubic-lstm}{{2.5}{9}{Cubic LSTM Architecture. (a) 3D structure of the CubicLSTM unit. (b) Topological diagram of the CubicLSTM unit. (c) Two-spatial-layer RNN composed of CubicLSTM units. The unit consists of three branches, a spatial (z-) branch for extracting and recognizing moving objects, a temporal (y-) branch for capturing and predicting motions, and an output (x-) branch for combining the first two branches to generate the predicted frames. Source:~\citet {CubicLSTMsVideoPrediction}\relax }{figure.caption.10}{}}
\citation{VideoFramePredictionByJointOptimizationWithSynthesisAndOpticalFlowEstimation}
\citation{VideoFramePredictionByJointOptimizationWithSynthesisAndOpticalFlowEstimation}
\citation{VideoFramePredictionByJointOptimizationWithSynthesisAndOpticalFlowEstimation}
\citation{FusionGRU}
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces Model Architecture. Source:\nobreakspace  {}\citet  {UnsupervisedVideoForecastingFlowParsingMechanism}\relax }}{10}{figure.caption.11}\protected@file@percent }
\newlabel{fig:unsupervised-video-forecasting-flow-parsing}{{2.6}{10}{Model Architecture. Source:~\citet {UnsupervisedVideoForecastingFlowParsingMechanism}\relax }{figure.caption.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces FPNet-OF model architecture. Source:\nobreakspace  {}\citet  {VideoFramePredictionByJointOptimizationWithSynthesisAndOpticalFlowEstimation}\relax }}{10}{figure.caption.12}\protected@file@percent }
\newlabel{fig:joint-optimization-synthesis-optical-flow}{{2.7}{10}{FPNet-OF model architecture. Source:~\citet {VideoFramePredictionByJointOptimizationWithSynthesisAndOpticalFlowEstimation}\relax }{figure.caption.12}{}}
\citation{FusionGRU}
\citation{FusionGRU}
\citation{DualBranchSpatialTemporalLearningNetworkVideoPrediction}
\citation{DualBranchSpatialTemporalLearningNetworkVideoPrediction}
\citation{DualBranchSpatialTemporalLearningNetworkVideoPrediction}
\citation{SurveyDLOD}
\citation{SurveyDLOD}
\@writefile{lof}{\contentsline {figure}{\numberline {2.8}{\ignorespaces Fusion-GRU model architecture. Source:\nobreakspace  {}\citet  {FusionGRU}\relax }}{11}{figure.caption.13}\protected@file@percent }
\newlabel{fig:fusion-gru}{{2.8}{11}{Fusion-GRU model architecture. Source:~\citet {FusionGRU}\relax }{figure.caption.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.9}{\ignorespaces Dual-Branch Spatial-Temporal Learning Network. Source:\nobreakspace  {}\citet  {DualBranchSpatialTemporalLearningNetworkVideoPrediction}\relax }}{11}{figure.caption.14}\protected@file@percent }
\newlabel{fig:dual-branch-spatial-temporal-learning-network}{{2.9}{11}{Dual-Branch Spatial-Temporal Learning Network. Source:~\citet {DualBranchSpatialTemporalLearningNetworkVideoPrediction}\relax }{figure.caption.14}{}}
\citation{DBLP:journals/corr/SrivastavaMS15}
\citation{CubicLSTMsVideoPrediction}
\citation{DBLP:journals/corr/SrivastavaMS15}
\citation{DBLP:journals/corr/SrivastavaMS15}
\citation{DBLP:journals/corr/FinnGL16}
\citation{CubicLSTMsVideoPrediction}
\@writefile{lof}{\contentsline {figure}{\numberline {2.10}{\ignorespaces Dual-Branch Spatial-Temporal Learning Network. Source:\nobreakspace  {}\citet  {SurveyDLOD}\relax }}{12}{figure.caption.15}\protected@file@percent }
\newlabel{fig:dual-branch-inter-intra-frames}{{2.10}{12}{Dual-Branch Spatial-Temporal Learning Network. Source:~\citet {SurveyDLOD}\relax }{figure.caption.15}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.11}{\ignorespaces 2-digit Moving MNIST data by\nobreakspace  {}\citet  {DBLP:journals/corr/SrivastavaMS15}\relax }}{12}{figure.caption.16}\protected@file@percent }
\newlabel{fig:moving-mnist}{{2.11}{12}{2-digit Moving MNIST data by~\citet {DBLP:journals/corr/SrivastavaMS15}\relax }{figure.caption.16}{}}
\citation{DBLP:journals/corr/EitelHB17}
\citation{DBLP:journals/corr/EitelHB17}
\citation{KTH}
\citation{CubicLSTMsVideoPrediction}
\citation{KTH}
\citation{KTH}
\citation{UCFSport}
\citation{DualBranchSpatialTemporalLearningNetworkVideoPrediction}
\citation{UCFSport}
\citation{UCFSport}
\@writefile{lof}{\contentsline {figure}{\numberline {2.12}{\ignorespaces Robotic Pushing Dataset. Source:\nobreakspace  {}\citet  {DBLP:journals/corr/EitelHB17}\relax }}{13}{figure.caption.17}\protected@file@percent }
\newlabel{fig:robotic-pushing-dataset}{{2.12}{13}{Robotic Pushing Dataset. Source:~\citet {DBLP:journals/corr/EitelHB17}\relax }{figure.caption.17}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.13}{\ignorespaces KTH Action Dataset. Source:\nobreakspace  {}\citet  {KTH}\relax }}{13}{figure.caption.18}\protected@file@percent }
\newlabel{fig:kth-action-dataset}{{2.13}{13}{KTH Action Dataset. Source:~\citet {KTH}\relax }{figure.caption.18}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.14}{\ignorespaces UCF Sports Dataset. Source:\nobreakspace  {}\citet  {UCFSport}\relax }}{13}{figure.caption.19}\protected@file@percent }
\newlabel{fig:ucf-sports-dataset}{{2.14}{13}{UCF Sports Dataset. Source:~\citet {UCFSport}\relax }{figure.caption.19}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Methodology}{14}{chapter.3}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Dataset Configuration}{14}{section.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}Provided Dataset Description}{14}{subsection.3.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.2}Data Annotation}{14}{subsection.3.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Framework Design}{15}{section.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Model Design}{15}{section.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Algorithm Design}{15}{section.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Experiment Design}{16}{chapter.4}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Experiment Environment}{16}{section.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Comparison Experiments}{16}{section.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Results and Discussion}{17}{chapter.5}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Data Description}{17}{section.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Experiment Results}{17}{section.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Testing Visualisation}{17}{section.5.3}\protected@file@percent }
\bibstyle{abbrvnat}
\bibdata{LaTeX,CUCitations}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Results and Discussion}{18}{chapter.6}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\bibcite{Alahi2016}{{1}{2016}{{Alahi et~al.}}{{Alahi, Goel, Ramanathan, Robicquet, Fei-Fei, and Savarese}}}
\bibcite{iata2019guidance}{{2}{2019}{{Association}}{{}}}
\bibcite{Bennett1991}{{3}{1991}{{Bennett et~al.}}{{Bennett, Shiu, and Leahy}}}
\bibcite{blakey2011aviation}{{4}{2011}{{Blakey et~al.}}{{Blakey, Rye, and Wilson}}}
\bibcite{DBLP:journals/corr/abs-2004-10934}{{5}{2020}{{Bochkovskiy et~al.}}{{Bochkovskiy, Wang, and Liao}}}
\bibcite{Chen2011}{{6}{2011}{{Chen and Stettner}}{{}}}
\bibcite{SurveyVisualOT}{{7}{2022}{{Chen et~al.}}{{Chen, Wang, Zhao, Lv, and Niu}}}
\bibcite{chen2023yoloms}{{8}{2023}{{Chen et~al.}}{{Chen, Yuan, Wu, Wang, Hou, and Cheng}}}
\bibcite{SurveySmallObjectDetection}{{9}{2023}{{Cheng et~al.}}{{Cheng, Yuan, Yao, Yan, Zeng, Xie, and Han}}}
\bibcite{DBLP:journals/corr/DaiLHS16}{{10}{2016}{{Dai et~al.}}{{Dai, Li, He, and Sun}}}
\bibcite{DBLP:journals/corr/EitelHB17}{{11}{2017}{{Eitel et~al.}}{{Eitel, Hauff, and Burgard}}}
\bibcite{huggingface2023objectdetection}{{12}{2023}{{Face}}{{}}}
\bibcite{CubicLSTMsVideoPrediction}{{13}{2019}{{Fan et~al.}}{{Fan, Zhu, and Yang}}}
\@writefile{toc}{\contentsline {chapter}{References}{19}{chapter*.20}\protected@file@percent }
\bibcite{DBLP:journals/corr/FinnGL16}{{14}{2016}{{Finn et~al.}}{{Finn, Goodfellow, and Levine}}}
\bibcite{DBLP:journals/corr/abs-2107-08430}{{15}{2021}{{Ge et~al.}}{{Ge, Liu, Wang, Li, and Sun}}}
\bibcite{DBLP:journals/corr/Girshick15}{{16}{2015}{{Girshick}}{{}}}
\bibcite{DBLP:journals/corr/GirshickDDM13}{{17}{2013}{{Girshick et~al.}}{{Girshick, Donahue, Darrell, and Malik}}}
\bibcite{AARBinocularVision}{{18}{2023}{{Gong et~al.}}{{Gong, Liu, Xu, Xu, He, Zhang, and Rasol}}}
\bibcite{HowDoUGoWhere}{{19}{2022}{{Hong et~al.}}{{Hong, Martin, and Raubal}}}
\bibcite{DualBranchSpatialTemporalLearningNetworkVideoPrediction}{{20}{2024}{{Huang and Guan}}{{}}}
\bibcite{UnsupervisedVideoForecastingFlowParsingMechanism}{{21}{2024}{{Jin et~al.}}{{Jin, Song, Li, and Zhang}}}
\bibcite{YOLOv5Release}{{22}{2022}{{Jocher}}{{}}}
\bibcite{YOLOv8}{{23}{2023}{{Jocher}}{{}}}
\bibcite{SurveyDLOD}{{24}{2022}{{Kang et~al.}}{{Kang, Tariq, Oh, and Woo}}}
\bibcite{FusionGRU}{{25}{2024}{{Karim et~al.}}{{Karim, Qin, and Wang}}}
\bibcite{kazda2015airport}{{26}{2015}{{Kazda and Caves}}{{}}}
\bibcite{DatasetAGR}{{27}{2023}{{Kuang et~al.}}{{Kuang, Barnes, Tang, and Jenkins}}}
\bibcite{SurveyTransformersSingleOT}{{28}{2023}{{Kugarajeevan et~al.}}{{Kugarajeevan, Kokul, Ramanan, and Fernando}}}
\bibcite{FFPSpaceSystemVehicles}{{29}{2022}{{Lee et~al.}}{{Lee, Jeon, Han, and Jeong}}}
\bibcite{li2023yolov6}{{30}{2023}{{Li et~al.}}{{Li, Li, Geng, Jiang, Cheng, Zhang, Ke, Xu, and Chu}}}
\bibcite{lin2018focal}{{31}{2018}{{Lin et~al.}}{{Lin, Goyal, Girshick, He, and Dollár}}}
\bibcite{OverviewCorrelationAlgoOT}{{32}{2021}{{Liu et~al.}}{{Liu, Liu, Srivastava, Połap, and Woźniak}}}
\bibcite{DBLP:journals/corr/LiuAESR15}{{33}{2015}{{Liu et~al.}}{{Liu, Anguelov, Erhan, Szegedy, Reed, Fu, and Berg}}}
\bibcite{VideoFramePredictionByJointOptimizationWithSynthesisAndOpticalFlowEstimation}{{34}{2023}{{Ranjan et~al.}}{{Ranjan, Bhandari, Kim, and Kim}}}
\bibcite{DBLP:journals/corr/RedmonF16}{{35}{2016}{{Redmon and Farhadi}}{{}}}
\bibcite{DBLP:journals/corr/RedmonDGF15}{{36}{2015}{{Redmon et~al.}}{{Redmon, Divvala, Girshick, and Farhadi}}}
\bibcite{Ren2017}{{37}{2017}{{Ren et~al.}}{{Ren, He, Girshick, and Sun}}}
\bibcite{UCFSport}{{38}{2008}{{Rodriguez et~al.}}{{Rodriguez, Ahmed, and Shah}}}
\bibcite{sati2019aircraft}{{39}{2019}{{Sati et~al.}}{{Sati, Singh, and Yadav}}}
\bibcite{KTH}{{40}{2004}{{Schuldt et~al.}}{{Schuldt, Laptev, and Caputo}}}
\bibcite{Schultz1986}{{41}{1986}{{Schultz}}{{}}}
\bibcite{ConvLSTM}{{42}{2015}{{Shi et~al.}}{{Shi, Chen, Wang, Yeung, Wong, and Woo}}}
\bibcite{DBLP:journals/corr/SrivastavaMS15}{{43}{2015}{{Srivastava et~al.}}{{Srivastava, Mansimov, and Salakhutdinov}}}
\bibcite{Vaswani2017}{{44}{2017}{{Vaswani et~al.}}{{Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Łukasz Kaiser, and Polosukhin}}}
\bibcite{wang2024yolov10}{{45}{2024{}}{{Wang et~al.}}{{Wang, Chen, Liu, Chen, Lin, Han, and Ding}}}
\bibcite{wang2023goldyolo}{{46}{2023}{{Wang et~al.}}{{Wang, He, Nie, Guo, Liu, Han, and Wang}}}
\bibcite{wang2024yolov9}{{47}{2024{}}{{Wang et~al.}}{{Wang, Yeh, and Liao}}}
\bibcite{AARCNN}{{48}{2017}{{Wang et~al.}}{{Wang, Dong, Kong, Li, and Zhang}}}
\bibcite{SmallObjectDetectionPositonPrediction}{{49}{2021}{{Wu and Xu}}{{}}}
\bibcite{xu2022ppyoloe}{{50}{2022}{{Xu et~al.}}{{Xu, Wang, Lv, Chang, Cui, Deng, Wang, Dang, Wei, Du, and Lai}}}
\bibcite{xu2023damoyolo}{{51}{2023}{{Xu et~al.}}{{Xu, Jiang, Chen, Huang, Zhang, and Sun}}}
\bibcite{ODNetworkUAVCNNTransformer}{{52}{2023}{{Ye et~al.}}{{Ye, Qin, Zhao, Gao, Deng, and Ouyang}}}
\bibcite{HybridDatasetAGRV2}{{53}{2023}{{Yildirim and Rana}}{{}}}
\bibcite{AGRPoseEstimation}{{54}{2021}{{Yildirim et~al.}}{{Yildirim, Rana, and Tang}}}
\bibcite{HybridDatasetAGRV1}{{55}{2023}{{Yildirim et~al.}}{{Yildirim, Rana, and Tang}}}
\bibcite{SurveyModernODModels}{{56}{2022}{{Zaidi et~al.}}{{Zaidi, Ansari, Aslam, Kanwal, Asghar, and Lee}}}
\bibcite{SuveyAdvancesSingleOTMethods}{{57}{2021}{{Zhang et~al.}}{{Zhang, Wang, Liu, Zhang, and Chen}}}
\bibcite{AAREKF}{{58}{2017}{{Zhong et~al.}}{{Zhong, Li, Wang, and Su}}}
\gdef \@abspage@last{30}
